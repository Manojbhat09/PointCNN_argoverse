{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing /home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/pythreejs/static -> jupyter-threejs\n",
      "Up to date: /home/kartik/.local/share/jupyter/nbextensions/jupyter-threejs/three.js\n",
      "Up to date: /home/kartik/.local/share/jupyter/nbextensions/jupyter-threejs/extension.js\n",
      "Up to date: /home/kartik/.local/share/jupyter/nbextensions/jupyter-threejs/index.js.map\n",
      "Up to date: /home/kartik/.local/share/jupyter/nbextensions/jupyter-threejs/index.js\n",
      "- Validating: \u001b[32mOK\u001b[0m\n",
      "\n",
      "    To initialize this nbextension in the browser every time the notebook (or other app) loads:\n",
      "    \n",
      "          jupyter nbextension enable pythreejs --user --py\n",
      "    \n",
      "Enabling notebook extension jupyter-threejs/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension install --user --py pythreejs\n",
    "!jupyter nbextension enable --user --py pythreejs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_utils\n",
    "%matplotlib notebook\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import h5py\n",
    "import argparse\n",
    "import importlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.animation\n",
    "import scipy.spatial as spatial\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection, Line3DCollection\n",
    "\n",
    "from matplotlib import colors as mcolors\n",
    "from sklearn.cluster import DBSCAN\n",
    "from matplotlib import animation\n",
    "# import pptk\n",
    "import numpy as np\n",
    "import colorsys\n",
    "from datetime import datetime\n",
    "\n",
    "import argoverse\n",
    "from argoverse.data_loading.argoverse_tracking_loader import ArgoverseTrackingLoader\n",
    "from pyntcloud import PyntCloud\n",
    "from argoverse.map_representation.map_api import ArgoverseMap\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import object3d\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pythreejs import *\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from ipywidgets import HTML, Text, Output, VBox\n",
    "from traitlets import link, dlink\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBBox(bounding_box,C1,C2,C3,C4,C5,C6,C7,C8,color=\"yellow\"):\n",
    "    bounding_box.append(\n",
    "        {\n",
    "            \"color\":color,\n",
    "            \"vertices\":[C1,C2,C3,C4,C1]\n",
    "        })\n",
    "    bounding_box.append(\n",
    "        {\n",
    "            \"color\":color,\n",
    "            \"vertices\":[C1,C4,C8,C5,C1]\n",
    "        })\n",
    "    bounding_box.append(\n",
    "        {\n",
    "            \"color\":color,\n",
    "            \"vertices\":[C1,C2,C6,C5,C1]\n",
    "        })\n",
    "    bounding_box.append(\n",
    "        {\n",
    "            \"color\":color,\n",
    "            \"vertices\":[C2,C6,C7,C3,C2]\n",
    "        })\n",
    "    bounding_box.append(\n",
    "        {\n",
    "            \"color\":color,\n",
    "            \"vertices\":[C3,C7,C8,C4,C3]\n",
    "        })\n",
    "    return bounding_box\n",
    "\n",
    "\n",
    "def getCorners_argo(height,width,length,x,y,z,rotMat,rotation=True):\n",
    "\n",
    "    corners = np.array([[-length / 2, -length / 2, length / 2, length / 2, -length / 2, -length / 2, length / 2, length / 2],\n",
    "                        [width / 2, -width / 2, -width / 2, width / 2, width / 2, -width / 2, -width / 2, width / 2],\n",
    "                        [-height/2, -height/2, -height/2, -height/2, height/2, height/2,  height/2, height/2]])\n",
    "    if rotation:\n",
    "        cornersPos = (np.dot(rotMat,corners)+np.tile([x,y,z],(8,1)).T).transpose()\n",
    "        corner1,corner2,corner3,corner4,corner5,corner6,corner7,corner8 = cornersPos[0],cornersPos[1],cornersPos[2],cornersPos[3],cornersPos[4],cornersPos[5],cornersPos[6],cornersPos[7]\n",
    "    else:\n",
    "        cornersPos = (corners + np.tile([x,y,z],(8,1)).T).transpose()\n",
    "        corner1,corner2,corner3,corner4,corner5,corner6,corner7,corner8 = cornersPos[0],cornersPos[1],cornersPos[2],cornersPos[3],cornersPos[4],cornersPos[5],cornersPos[6],cornersPos[7]\n",
    "    \n",
    "    return list(corner1),list(corner2),list(corner3),list(corner4),list(corner5),list(corner6),list(corner7),list(corner8)\n",
    "\n",
    "lines = []\n",
    "lines.append(\n",
    "    {\n",
    "        \"color\":'red', #x\n",
    "        \"vertices\":[[5,0,0],[0,0,0],[0,0,0]]\n",
    "    })\n",
    "lines.append(\n",
    "    {\n",
    "        \"color\":'green', #y\n",
    "        \"vertices\":[[0,0,0],[0,5,0],[0,0,0]]\n",
    "    })\n",
    "lines.append(\n",
    "    {\n",
    "        \"color\":'blue', #z\n",
    "        \"vertices\":[[0,0,0],[0,0,0],[0,0,5]]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=None, epochs=None, filelist=None, filelist_val=None, load_ckpt=None, log='log.txt', model=None, no_code_backup=False, no_timestamp_folder=False, save_folder=None, setting=None)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--filelist',  '-t', help='Path to training set ground truth (.txt)')\n",
    "parser.add_argument('--filelist_val', '-v', help='Path to validation set ground truth (.txt)')\n",
    "parser.add_argument('--load_ckpt', '-l', help='Path to a check point file for load')\n",
    "parser.add_argument('--save_folder', '-s', help='Path to folder for saving check points and summary')\n",
    "parser.add_argument('--model', '-m', help='Model to use')\n",
    "parser.add_argument('--setting', '-x', help='Setting to use')\n",
    "parser.add_argument('--epochs', help='Number of training epochs (default defined in setting)', type=int)\n",
    "parser.add_argument('--batch_size', help='Batch size (default defined in setting)', type=int)\n",
    "parser.add_argument('--log', help='Log to FILE in save folder; use - for stdout (default is log.txt)', metavar='FILE', default='log.txt')\n",
    "parser.add_argument('--no_timestamp_folder', help='Dont save to timestamp folder', action='store_true')\n",
    "parser.add_argument('--no_code_backup', help='Dont backup code', action='store_true')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# if not args.no_timestamp_folder:\n",
    "#     time_string = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "#     root_folder = os.path.join(args.save_folder, '%s_%s_%s_%d' % (args.model, args.setting, time_string, os.getpid()))\n",
    "# else:\n",
    "#     root_folder = args.save_folder\n",
    "# if not os.path.exists(root_folder):\n",
    "#     os.makedirs(root_folder)\n",
    "print(args)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "dataset_dir  = os.path.join(cwd, \"data\", \"KITTI\")\n",
    "filelist = os.path.join(dataset_dir, \"ImageSets\")\n",
    "train_data = os.path.join(filelist, \"train.txt\")\n",
    "\n",
    "args = {\n",
    "        'repeat_num' : 1,\n",
    "        'setting' : \"kitti3d_x8_2048_fps\",\n",
    "        'save_ply' : False,\n",
    "        'model' : \"pointcnn_seg\",\n",
    "        'max_point_num' : 10000,\n",
    "        'filelist' : train_data\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = importlib.import_module(args['model'])\n",
    "setting_path = os.path.join(cwd, args['model'])\n",
    "sys.path.append(setting_path)\n",
    "setting = importlib.import_module(args['setting'])\n",
    "\n",
    "num_epochs = setting.num_epochs\n",
    "batch_size = setting.batch_size\n",
    "sample_num = setting.sample_num\n",
    "step_val = setting.step_val\n",
    "label_weights_list = setting.label_weights\n",
    "rotation_range = setting.rotation_range\n",
    "rotation_range_val = setting.rotation_range_val\n",
    "scaling_range = setting.scaling_range\n",
    "scaling_range_val = setting.scaling_range_val\n",
    "jitter = setting.jitter\n",
    "jitter_val = setting.jitter_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prepare for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(block_size=5.0, folder=None, grid_size=0.1, max_point_num=8192, save_ply=False)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os \n",
    "import numpy as np\n",
    "import glob \n",
    "import datetime\n",
    "import time \n",
    "import argoverse\n",
    "from argoverse.data_loading.argoverse_tracking_loader import ArgoverseTrackingLoader\n",
    "from pyntcloud import PyntCloud\n",
    "from argoverse.map_representation.map_api import ArgoverseMap\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--folder', '-f', help='Path to data folder')\n",
    "parser.add_argument('--max_point_num', '-m', help='Max point number of each sample', type=int, default=8192)\n",
    "parser.add_argument('--block_size', '-b', help='Block size', type=float, default=5.0)\n",
    "parser.add_argument('--grid_size', '-g', help='Grid size', type=float, default=0.1)\n",
    "parser.add_argument('--save_ply', '-s', help='Convert .pts to .ply', action='store_true')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "print(args)\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "root = args.folder if args.folder else os.path.join(cwd, \"data\", \"Argo\") +\"/\"\n",
    "save_h5_root =  os.path.join(cwd, \"data\", \"Argo_h5\")\n",
    "\n",
    "if args.save_ply:\n",
    "    data_center = np.zeros((batch_size, max_point_num, 3))\n",
    "    \n",
    "folders = [os.path.join(root, folder) for folder in ['train', 'val', 'test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/kartik/DL_model/PointCNN/data/Argo/train',\n",
       " '/home/kartik/DL_model/PointCNN/data/Argo/val',\n",
       " '/home/kartik/DL_model/PointCNN/data/Argo/test']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________SPLIT IS : train ______________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:argoverse.data_loading.synchronization_database:No corresponding stereo image at 315967661819717000: 176.11196 > 100.0 ms\n",
      "WARNING:argoverse.data_loading.synchronization_database:No corresponding stereo image at 315967661819717000: 176.107928 > 100.0 ms\n",
      "WARNING:argoverse.data_loading.synchronization_database:No corresponding stereo image at 315967953120029000: 170.267688 > 100.0 ms\n",
      "WARNING:argoverse.data_loading.synchronization_database:No corresponding stereo image at 315967953120029000: 170.390336 > 100.0 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample from lidar pathsL  /home/kartik/DL_model/PointCNN/data/Argo/train/3d20ae25-5b29-320d-8bae-f03e9dc177b9/lidar/PC_315975023020283000.ply\n",
      "image list sample is:  ['3d20ae25-5b29-320d-8bae-f03e9dc177b9', '315975023020283000']\n",
      "len of list is same \n",
      "\n"
     ]
    }
   ],
   "source": [
    "split = 'train'\n",
    "root_dir = root\n",
    "is_test = (split == 'test')\n",
    "lidar_pathlist = []\n",
    "label_pathlist = []\n",
    "actual_idx_list = []\n",
    "logidx_to_count_map= {}\n",
    "log_to_count_map= {}\n",
    "\n",
    "print(\"____________SPLIT IS : {} ______________\".format(split))\n",
    "if split == 'train':\n",
    "    imageset_dir = os.path.join(root_dir,split)\n",
    "    splitname = lambda x: [x[len(imageset_dir+\"/\"):-4].split(\"/\")[0], x[len(imageset_dir+\"/\"):-4].split(\"/\")[2].split(\"_\")[1]]\n",
    "    data_loader = ArgoverseTrackingLoader(os.path.join(root_dir,split))\n",
    "    log_list = data_loader.log_list\n",
    "    path_count = 0\n",
    "    for log_id, log in enumerate(log_list):\n",
    "        lidar_lst = data_loader.get(log).lidar_list\n",
    "        lidar_pathlist.extend(lidar_lst)\n",
    "        label_pathlist.extend(data_loader.get(log).label_list)\n",
    "        actual_idx_list.extend([splitname(each) for each in lidar_lst])\n",
    "        logidx_to_count_map[log_id] = np.arange(path_count, path_count + len(lidar_lst))\n",
    "        log_to_count_map[log] = np.arange(path_count, path_count + len(lidar_lst))\n",
    "        path_count+=len(lidar_lst)\n",
    "    assert len(lidar_pathlist) == len(label_pathlist)\n",
    "\n",
    "elif split == 'test':\n",
    "    imageset_dir = os.path.join(root_dir,split)\n",
    "    splitname = lambda x: [x[len(imageset_dir+\"/\"):-4].split(\"/\")[0], x[len(imageset_dir+\"/\"):-4].split(\"/\")[2].split(\"_\")[1]]\n",
    "    print(\"______________image set dir_________\", imageset_dir)\n",
    "    data_loader = ArgoverseTrackingLoader(os.path.join(root_dir,split))\n",
    "    log_list = data_loader.log_list\n",
    "    path_count = 0\n",
    "    for log in log_list:\n",
    "        lidar_pathlist.extend(data_loader.get(log).lidar_list)\n",
    "        actual_idx_list.extend([splitname(each) for each in lidar_lst])\n",
    "        logidx_to_count_map[log_id] = np.arange(path_count, path_count + len(lidar_lst))\n",
    "        log_to_count_map[log] = np.arange(path_count, path_count + len(lidar_lst))\n",
    "        path_count+=len(lidar_lst)\n",
    "    print(\"The lidar list len is : \",len(self.lidar_pathlist))\n",
    "    label_pathlist = None\n",
    "    \n",
    "    \n",
    "am = ArgoverseMap()\n",
    "calib_file = data_loader.calib_filename\n",
    "print(\"sample from lidar pathsL \",lidar_pathlist[0])\n",
    "\n",
    "num_sample = len(lidar_pathlist)\n",
    "image_idx_list = np.arange(num_sample)\n",
    "print(\"image list sample is: \", actual_idx_list[0])\n",
    "\n",
    "if len(lidar_pathlist) > len(actual_idx_list):\n",
    "    print(\"There is length difference between lidar and actual files : \", lidar_pathlist, \" \",actual_idx_list )\n",
    "else:\n",
    "    print(\"len of list is same \\n\")\n",
    "\n",
    "argo_to_kitti = np.array([[6.927964e-03, -9.999722e-01, -2.757829e-03],\n",
    "                               [-1.162982e-03, 2.749836e-03, -9.999955e-01],\n",
    "                               [9.999753e-01, 6.931141e-03, -1.143899e-03]])\n",
    "\n",
    "ground_removal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import object3d\n",
    "import json\n",
    "from functools import reduce\n",
    "\n",
    "def get_objects_from_label(label_file):\n",
    "    # Opens a label file, and passes the object to Object3d object, Read the json GT labels\n",
    "    \n",
    "    f = open(label_file)\n",
    "    label_data = json.load(f) \n",
    "    objects = [object3d.Object3d(data) for data in label_data]\n",
    "    return objects\n",
    "\n",
    "def filter_pointcloud(bbox, pointcloud):\n",
    "    theta = bbox.ry #[\"angle\"]\n",
    "    transformed_pointcloud = homogeneous_transformation(pointcloud[:, :3], bbox.pos[:3], -theta)#[\"center\"]\n",
    "    if bbox.l > bbox.w:\n",
    "        length = bbox.l\n",
    "        width = bbox.w\n",
    "    else:\n",
    "        length = bbox.w\n",
    "        width = bbox.l\n",
    "            \n",
    "#     indices = np.intersect1d(np.where(np.abs(transformed_pointcloud[:,0]) <= width/2)[0], \n",
    "#                              np.where(np.abs(transformed_pointcloud[:,1]) <= length/2)[0])\n",
    "    \n",
    "    indices = reduce(np.intersect1d, (np.where(np.abs(transformed_pointcloud[:,0]) <= width/2)[0], \n",
    "                            np.where(np.abs(transformed_pointcloud[:,1]) <= length/2)[0],\n",
    "                            np.where(np.abs(transformed_pointcloud[:,2]) <= bbox.h/2)[0]))\n",
    "    return indices, pointcloud[indices,:], transformed_pointcloud, bbox\n",
    "\n",
    "def homogeneous_transformation(points, translation, theta):\n",
    "    return (points[:, :3] - translation).dot(rotation_matrix_2D(theta).T)\n",
    "\n",
    "def rotation_matrix_2D(theta):\n",
    "    return np.array([[np.cos(theta), -np.sin(theta), 0], \n",
    "                     [np.sin(theta), np.cos(theta), 0],\n",
    "                     [0,0,1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data_input ,labels,  max_point_num):\n",
    "    batch_size = 2048\n",
    "    block_size = 1000\n",
    "    grid_size = 0.25\n",
    "        \n",
    "    assert len(labels) == data_input.shape[0]\n",
    "#     data = np.zeros((batch_size, max_point_num, 4))\n",
    "    data = np.zeros((batch_size, max_point_num, 3))\n",
    "    data_num = np.zeros((batch_size), dtype=np.int32)\n",
    "    label = np.zeros((batch_size), dtype=np.int32)\n",
    "    label_seg = np.zeros((batch_size, max_point_num), dtype=np.int32)\n",
    "    indices_split_to_full = np.zeros((batch_size, max_point_num), dtype=np.int32)\n",
    "    \n",
    "    xyzi = data_input[:, :4]\n",
    "    \n",
    "    \n",
    "    indices_for_prediction = np.arange(xyzi.shape[0]) #(xyzi[:,0] >= -5 ).nonzero()[0]\n",
    "    #print(\"indices_for_prediction\", indices_for_prediction)\n",
    "    # Filter point only in front on of ego-sensors\n",
    "    xyzif =xyzi #= xyzi[xyzi[:,0] >= -5 ] \n",
    "    \n",
    "    all_label_pred = np.zeros((xyzi.shape[0]),dtype=int)\n",
    "    label_length = xyzif.shape[0]\n",
    "    xyz =xyzif[:,0:3]\n",
    "    \n",
    "            \n",
    "    xyz_min = np.amin(xyz, axis=0, keepdims=True)\n",
    "    xyz_max = np.amax(xyz, axis=0, keepdims=True)\n",
    "    block_size = (2 * (xyz_max[0, 0] - xyz_min[0, 0]), 2 * (xyz_max[0, 1] - xyz_min[0, 1]) ,  2 * (xyz_max[0, -1] - xyz_min[0, -1]))\n",
    "    \n",
    "    xyz_blocks = np.floor((xyz - xyz_min) / block_size).astype(np.int)\n",
    "\n",
    "    #print('{}-Collecting points belong to each block...'.format(datetime.now(), xyzrcof.shape[0]))\n",
    "    blocks, point_block_indices, block_point_counts = np.unique(xyz_blocks, return_inverse=True,\n",
    "                                                                return_counts=True, axis=0)\n",
    "    block_point_indices = np.split(np.argsort(point_block_indices), np.cumsum(block_point_counts[:-1]))\n",
    "    #print('{}-{} is split into {} blocks.'.format(datetime.now(), dataset, blocks.shape[0]))\n",
    "\n",
    "    block_to_block_idx_map = dict()\n",
    "    for block_idx in range(blocks.shape[0]):\n",
    "        block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "        block_to_block_idx_map[(block[0], block[1])] = block_idx\n",
    "\n",
    "    # merge small blocks into one of their big neighbors\n",
    "    block_point_count_threshold = max_point_num / 3\n",
    "    #print(\"block_point_count_threshold\",block_point_count_threshold)\n",
    "    nbr_block_offsets = [(0, 1), (1, 0), (0, -1), (-1, 0), (-1, 1), (1, 1), (1, -1), (-1, -1)]\n",
    "    block_merge_count = 0\n",
    "    for block_idx in range(blocks.shape[0]):\n",
    "        if block_point_counts[block_idx] >= block_point_count_threshold:\n",
    "            #print(block_idx, block_point_counts[block_idx])\n",
    "\n",
    "            continue\n",
    "\n",
    "\n",
    "        block = (blocks[block_idx][0], blocks[block_idx][1])\n",
    "        for x, y in nbr_block_offsets:\n",
    "            nbr_block = (block[0] + x, block[1] + y)\n",
    "            if nbr_block not in block_to_block_idx_map:\n",
    "                continue\n",
    "\n",
    "            nbr_block_idx = block_to_block_idx_map[nbr_block]\n",
    "            if block_point_counts[nbr_block_idx] < block_point_count_threshold:\n",
    "                continue\n",
    "\n",
    "\n",
    "            #print(block_idx, nbr_block_idx, block_point_counts[nbr_block_idx])\n",
    "\n",
    "            block_point_indices[nbr_block_idx] = np.concatenate(\n",
    "                [block_point_indices[nbr_block_idx], block_point_indices[block_idx]], axis=-1)\n",
    "            block_point_indices[block_idx] = np.array([], dtype=np.int)\n",
    "            block_merge_count = block_merge_count + 1\n",
    "            break\n",
    "    #print('{}-{} of {} blocks are merged.'.format(datetime.now(), block_merge_count, blocks.shape[0]))\n",
    "\n",
    "    idx_last_non_empty_block = 0\n",
    "    for block_idx in reversed(range(blocks.shape[0])):\n",
    "        if block_point_indices[block_idx].shape[0] != 0:\n",
    "            idx_last_non_empty_block = block_idx\n",
    "            break\n",
    "\n",
    "    # uniformly sample each block\n",
    "    for block_idx in range(idx_last_non_empty_block + 1):\n",
    "        point_indices = block_point_indices[block_idx]\n",
    "        if point_indices.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        #print(block_idx, point_indices.shape)\n",
    "        block_points = xyz[point_indices]\n",
    "        block_min = np.amin(block_points, axis=0, keepdims=True)\n",
    "        xyz_grids = np.floor((block_points - block_min) / grid_size).astype(np.int)\n",
    "        grids, point_grid_indices, grid_point_counts = np.unique(xyz_grids, return_inverse=True,\n",
    "                                                                 return_counts=True, axis=0)\n",
    "        grid_point_indices = np.split(np.argsort(point_grid_indices), np.cumsum(grid_point_counts[:-1]))\n",
    "        grid_point_count_avg = int(np.average(grid_point_counts))\n",
    "        point_indices_repeated = []\n",
    "        for grid_idx in range(grids.shape[0]):\n",
    "            point_indices_in_block = grid_point_indices[grid_idx]\n",
    "            repeat_num = math.ceil(grid_point_count_avg / point_indices_in_block.shape[0])\n",
    "            if repeat_num > 1:\n",
    "                point_indices_in_block = np.repeat(point_indices_in_block, repeat_num)\n",
    "                np.random.shuffle(point_indices_in_block)\n",
    "                point_indices_in_block = point_indices_in_block[:grid_point_count_avg]\n",
    "            point_indices_repeated.extend(list(point_indices[point_indices_in_block]))\n",
    "        block_point_indices[block_idx] = np.array(point_indices_repeated)\n",
    "        block_point_counts[block_idx] = len(point_indices_repeated)\n",
    "\n",
    "    idx = 0\n",
    "    for block_idx in range(idx_last_non_empty_block + 1):\n",
    "        point_indices = block_point_indices[block_idx]\n",
    "        if point_indices.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        block_point_num = point_indices.shape[0]\n",
    "        block_split_num = int(math.ceil(block_point_num * 1.0 / max_point_num))\n",
    "        point_num_avg = int(math.ceil(block_point_num * 1.0 / block_split_num))\n",
    "        point_nums = [point_num_avg] * block_split_num\n",
    "        point_nums[-1] = block_point_num - (point_num_avg * (block_split_num - 1))\n",
    "        starts = [0] + list(np.cumsum(point_nums))\n",
    "\n",
    "        np.random.shuffle(point_indices)\n",
    "        block_points = xyz[point_indices]\n",
    "\n",
    "\n",
    "        block_min = np.amin(block_points, axis=0, keepdims=True)\n",
    "        block_max = np.amax(block_points, axis=0, keepdims=True)\n",
    "        #block_center = (block_min + block_max) / 2\n",
    "        #block_center[0][-1] = block_min[0][-1]\n",
    "        #block_points = block_points - block_center  # align to block bottom center\n",
    "        x, y, z = np.split(block_points, (1, 2), axis=-1)\n",
    "\n",
    "#         block_xzyrgbi = np.concatenate([x, z, y, i[point_indices]], axis=-1)\n",
    "        block_xzyrgbi = np.concatenate([x, y, z], axis=-1)\n",
    "        block_labels = labels[point_indices]\n",
    "\n",
    "        for block_split_idx in range(block_split_num):\n",
    "            start = starts[block_split_idx]\n",
    "            point_num = point_nums[block_split_idx]\n",
    "            #print(block_split_num, block_split_idx, point_num )\n",
    "\n",
    "\n",
    "\n",
    "            end = start + point_num\n",
    "            idx_in_batch = idx % batch_size\n",
    "            data[idx_in_batch, 0:point_num, ...] = block_xzyrgbi[start:end, :]\n",
    "            data_num[idx_in_batch] = point_num\n",
    "            \n",
    "            label_seg[idx_in_batch, 0:point_num] = block_labels[start:end]\n",
    "            \n",
    "            indices_split_to_full[idx_in_batch, 0:point_num] = point_indices[start:end]\n",
    "\n",
    "            #print(\"indices_split_to_full\", idx_in_batch, point_num, indices_split_to_full)\n",
    "\n",
    "            if  (block_idx == idx_last_non_empty_block and block_split_idx == block_split_num - 1): #Last iteration\n",
    "\n",
    "                item_num = idx_in_batch + 1\n",
    "                \n",
    "            idx = idx + 1\n",
    "            \n",
    "    return label_length, data, data_num, label_seg, indices_split_to_full, item_num, all_label_pred, indices_for_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9c05aefb3b4b84a839236fe40a5170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "object_dict = {\n",
    "    'BICYCLE' : 3,\n",
    "    'PEDESTRIAN': 1,\n",
    "    'ON_ROAD_OBSTACLE': 4,\n",
    "    'VEHICLE':2\n",
    "}\n",
    "\n",
    "\n",
    "max_point_num = args.max_point_num\n",
    "\n",
    "batch_size = 2048\n",
    "data = np.zeros((batch_size, max_point_num, 5))\n",
    "data_num = np.zeros((batch_size), dtype=np.int32)\n",
    "label = np.zeros((batch_size), dtype=np.int32)\n",
    "label_seg = np.zeros((batch_size, max_point_num), dtype=np.int32)\n",
    "indices_split_to_full = np.zeros((batch_size, max_point_num), dtype=np.int32)\n",
    "\n",
    "data_all= list()\n",
    "data_num_all= list()\n",
    "label_all= list()\n",
    "label_seg_all= list()\n",
    "indices_all= list()\n",
    "\n",
    "id_h5 = 0\n",
    "h5_batches = 200\n",
    "\n",
    "for data_idx, dataset_file in enumerate(tqdm(sorted(lidar_pathlist))):\n",
    "    \n",
    "    # Get lidar points\n",
    "    actual_index = actual_idx_list[data_idx]\n",
    "    data_points = PyntCloud.from_file(dataset_file)\n",
    "    pts_lidar = data_points\n",
    "    \n",
    "    folder = actual_index[0]\n",
    "    dataset = int(actual_index[1])\n",
    "    \n",
    "    ########################################################\n",
    "    log, idx = actual_index[0], int(actual_index[1])\n",
    "    lidar_file = dataset_file\n",
    "\n",
    "    argoverse_data = data_loader.get(log)\n",
    "    city_name = argoverse_data.city_name\n",
    "    \n",
    "    log_dataidx_list = log_to_count_map[log]\n",
    "    log_data_idx = log_dataidx_list.tolist().index(data_idx)\n",
    "    \n",
    "    lidar_pts = argoverse_data.get_lidar(log_data_idx)\n",
    "    city_to_egovehicle_se3 = argoverse_data.get_pose(log_data_idx)\n",
    "    roi_area_pts = city_to_egovehicle_se3.transform_point_cloud(lidar_pts) # more to city CS\n",
    "    roi_area_pts = am.remove_non_roi_points(roi_area_pts, city_name) # remove outside roi points\n",
    "    roi_area_pts = am.remove_ground_surface(roi_area_pts, city_name) # remove ground  \n",
    "    roi_area_pts = city_to_egovehicle_se3.inverse_transform_point_cloud(\n",
    "        roi_area_pts\n",
    "    )# Back to lidar cs\n",
    "\n",
    "    x = np.array(roi_area_pts[:,0])[:, np.newaxis]\n",
    "    y = np.array(roi_area_pts[:,1])[:, np.newaxis]\n",
    "    z = np.array(roi_area_pts[:,2])[:, np.newaxis]\n",
    "    pts_lidar = np.concatenate([x,y,z], axis = 1)\n",
    "    \n",
    "#     if ground_removal: \n",
    "#         pts_lidar = gs.ground_segmentation(pts_lidar)\n",
    "    ###########################################################\n",
    "#     pts_lidar = np.dot(argo_to_kitti,pts_lidar.T).T\n",
    "    \n",
    "    # Get objects\n",
    "    label_file = label_pathlist[data_idx]\n",
    "    objects = get_objects_from_label(label_file)\n",
    "    \n",
    "    bounding_box = list()\n",
    "    croppedpc = list()\n",
    "    label_indices = list()\n",
    "    instance_pts = list()\n",
    "    instance_indices = list()\n",
    "    class_labels = list()\n",
    "    \n",
    "#     label_mask_seg = np.zeros(len(pts_lidar))\n",
    "#     label_cls = np.zeros(label_mask_seg[-1], dtype = int ) \n",
    "#     label_cls[label_mask_seg] = 1 # feature 1\n",
    "#     label_pts = list(map(list,set(tuple(i) for i in croppedpc)))\n",
    "#     label_num = len(label_pts) # feature 2\n",
    "#     data_num = len(pts_lidar)\n",
    "    \n",
    "    labels = np.zeros(pts_lidar.shape[0])\n",
    "    rgb = np.zeros(pts_lidar.shape[0])\n",
    "    i = np.zeros(pts_lidar.shape[0])\n",
    "    bounding_box1 = []\n",
    "    bounding_box2 = []\n",
    "    \n",
    "    # Get labels\n",
    "    for each in objects:\n",
    "        bounding_box1 = createBBox(bounding_box1,*getCorners_argo(each.h,each.w,each.l, *np.dot(each.pos, argo_to_kitti), each.rot_mat_argo,rotation=False),color=\"yellow\")\n",
    "        object_id = object_dict[each.cls_type]\n",
    "        \n",
    "        each.pos = np.dot(each.pos, argo_to_kitti)\n",
    "        each.l = each.l + 0.1* each.l\n",
    "        each.w = each.w + 0.1* each.w\n",
    "        each.h = each.h + 0.1* each.h\n",
    "        out = filter_pointcloud(each, np.copy(pts_lidar))\n",
    "        \n",
    "#         croppedpc.extend(out[1].tolist() if type(out[1]) == np.ndarray else out[1])\n",
    "        label_indices.extend(out[0].tolist() if type(out[0]) == np.ndarray else out[0])\n",
    "        class_labels.extend( np.zeros(len(out[0]), dtype=int) + object_id )\n",
    "        labels[label_indices] = object_id\n",
    "        rgb[label_indices] = len(out[0])\n",
    "        i[label_indices] = object_id\n",
    "        \n",
    "        pc_trans = out[2]\n",
    "        bbox = out[3]\n",
    "        bounding_box2 = createBBox(bounding_box2,*getCorners_argo(each.h,each.w,each.l, *each.pos, each.rot_mat_argo,rotation=False),color=\"yellow\")\n",
    "        cropped_pc = out[1]\n",
    "        \n",
    "#         break\n",
    "        \n",
    "    xyzirgb = pts_lidar\n",
    "\n",
    "    data_full = np.hstack((pts_lidar, rgb[:,np.newaxis]))\n",
    "\n",
    "    label_length, data, data_num, label_seg, indices_split_to_full, item_num, all_label_pred, indices_for_prediction = data_preprocessing(data_full, labels, max_point_num)\n",
    "\n",
    "    data_data =data[0:item_num, ...].astype(np.float32) \n",
    "    data_num =data_num[0:item_num, ...] \n",
    "    indices_split_to_full = indices_split_to_full[0:item_num]\n",
    "    label_seg_data = label_seg[0:item_num]\n",
    "    batch_num = data.shape[0]\n",
    "\n",
    "    data_data = np.concatenate(data_data[0:6, :, :3])\n",
    "    label_data = np.concatenate(label_seg_data[0:6, :])\n",
    "        \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/pythreejs/traits.py:191: UserWarning: 64-bit data types not supported for WebGL data, casting to 32-bit.\n",
      "  warnings.warn('64-bit data types not supported for WebGL '\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531cc320c259427fa381b26aad5d770c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(1.9598288628764933, 47.76553760943052, 21.76…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338b5a6df6ae4477adb40ae021f0ba25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.02, max=0.2, step=0.0002), Label(value='Backgro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vizualize actual pointcloud with boxes\n",
    "\n",
    "new_pts = pd.DataFrame(np.array(pts_lidar)[:, 0:3], columns=['x', 'y', 'z'])\n",
    "cloud = PyntCloud(new_pts)\n",
    "cloud.plot(initial_point_size=0.02, polylines= lines+bounding_box1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa21b03acf364d5a948bfe73a34d6562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(1.170361952333853, -0.5527554605373521, 2.65…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22dfe2dd06cd4d459c5d7f4a8ffa8a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.02, max=0.2, step=0.0002), Label(value='Backgro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vizulaized cropped pointcloud with mod-boxes \n",
    "\n",
    "new_pts = pd.DataFrame(cropped_pc[:, 0:3], columns=['x', 'y', 'z'])\n",
    "cloud = PyntCloud(new_pts)\n",
    "cloud.plot(initial_point_size=0.02, polylines= lines+bounding_box2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2710435c174897a1df50826205a22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(1.6794755892224524, 47.725793096709424, 21.4…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa1a527c3174b82bed85368e4075def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.02, max=0.2, step=0.0002), Label(value='Backgro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vizualize segmented points\n",
    "\n",
    "data_pts = pts_lidar\n",
    "new_pts1 = pd.DataFrame(np.array(data_pts)[:, 0:3], columns=['x', 'y', 'z'])\n",
    "new_pts1[\"red\"]  = 255\n",
    "new_pts1[\"green\"]  = 121\n",
    "new_pts1[\"blue\"]  = 121\n",
    "labelled_pts = data_pts[label_indices]\n",
    "new_pts2 = pd.DataFrame(np.array(labelled_pts)[:, 0:3], columns=['x', 'y', 'z'])\n",
    "new_pts2[\"red\"]  = 255\n",
    "new_pts2[\"green\"]  = 255\n",
    "new_pts2[\"blue\"]  = 255\n",
    "cloud = PyntCloud(new_pts1.append(new_pts2))\n",
    "cloud.plot(initial_point_size=0.02, polylines= lines+bounding_box1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85cbbb05121941978e32ac29003e122b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(1.6794755892224524, 47.725793096709424, 21.4…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f59ce30cc14cd59b03dace0e567db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.02, max=0.2, step=0.0002), Label(value='Backgro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# enlarged bboxes\n",
    "\n",
    "# vizualize segmented points\n",
    "\n",
    "data_pts = pts_lidar\n",
    "new_pts1 = pd.DataFrame(np.array(data_pts)[:, 0:3], columns=['x', 'y', 'z'])\n",
    "new_pts1[\"red\"]  = 255\n",
    "new_pts1[\"green\"]  = 121\n",
    "new_pts1[\"blue\"]  = 121\n",
    "labelled_pts = data_pts[label_indices]\n",
    "new_pts2 = pd.DataFrame(np.array(labelled_pts)[:, 0:3], columns=['x', 'y', 'z'])\n",
    "new_pts2[\"red\"]  = 255\n",
    "new_pts2[\"green\"]  = 255\n",
    "new_pts2[\"blue\"]  = 255\n",
    "cloud = PyntCloud(new_pts1.append(new_pts2))\n",
    "cloud.plot(initial_point_size=0.02, polylines= lines+bounding_box2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "219998692d394afda9b265dbc014dd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(2.688401460647583, 48.19766020774841, 22.073…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c292927e40c4522af46189ef7a487be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.02, max=0.2, step=0.0002), Label(value='Backgro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vizualize processed points\n",
    "\n",
    "data_pts = data_data\n",
    "new_pts1 = pd.DataFrame(np.array(data_pts)[:, 0:3], columns=['x', 'y', 'z'])\n",
    "new_pts1[\"red\"]  = 255\n",
    "new_pts1[\"green\"]  = 121\n",
    "new_pts1[\"blue\"]  = 121\n",
    "labelled_pts = data_pts[label_data == 2]\n",
    "new_pts2 = pd.DataFrame(np.array(labelled_pts)[:, 0:3], columns=['x', 'y', 'z'])\n",
    "new_pts2[\"red\"]  = 255\n",
    "new_pts2[\"green\"]  = 255\n",
    "new_pts2[\"blue\"]  = 255\n",
    "cloud = PyntCloud(new_pts1.append(new_pts2))\n",
    "cloud.plot(initial_point_size=0.02, polylines= lines+bounding_box1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c80dd2e9a2f494386647aaaf6aa0d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(4.808792027944373, 206.71896873160773, 20.62…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2833ab7cb22e46778f3d4136bf2407dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.02, max=0.2, step=0.0002), Label(value='Backgro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vizualize transformed points\n",
    "\n",
    "new_pts = pd.DataFrame(np.array(pc_trans)[:, 0:3], columns=['x', 'y', 'z'])\n",
    "cloud = PyntCloud(new_pts)\n",
    "cloud.plot(initial_point_size=0.02, polylines= lines+bounding_box1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_h5 = os.path.join(\"/home/kartik/DL_model/PointCNN/data/Argo_h5/train/train_full_0.h5\")\n",
    "data = h5py.File(path_h5)\n",
    "h5_data = data['data'][:,:,0:3].astype(np.float32)\n",
    "h5_label = data['label_seg'][...].astype(np.int64)\n",
    "\n",
    "h5_data = np.concatenate(h5_data[0:6, :, :3])\n",
    "h5_label = np.concatenate(h5_label[0:6, :])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81125e4ed9ba43cc8f1bc812536f81c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(2.3492038249969482, 48.20943880081177, 21.81…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aacbe22e8d424e71bb30efa1cda32b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.02, max=0.2, step=0.0002), Label(value='Backgro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# vizualize h5 points\n",
    "\n",
    "data_pts = h5_data\n",
    "new_pts1 = pd.DataFrame(np.array(data_pts)[:, 0:3], columns=['x', 'z', 'y'])\n",
    "new_pts1[\"red\"]  = 255\n",
    "new_pts1[\"green\"]  = 121\n",
    "new_pts1[\"blue\"]  = 121\n",
    "labelled_pts = data_pts[h5_label == 8]\n",
    "new_pts2 = pd.DataFrame(np.array(labelled_pts)[:, 0:3], columns=['x','z', 'y'])\n",
    "new_pts2[\"red\"]  = 255\n",
    "new_pts2[\"green\"]  = 255\n",
    "new_pts2[\"blue\"]  = 255\n",
    "cloud = PyntCloud(new_pts1.append(new_pts2))\n",
    "cloud.plot(initial_point_size=0.02, polylines= lines+bounding_box1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9305"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_indices.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10713"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(label_data == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10715"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(h5_label == 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34190"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts_lidar.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49152, 3)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 49152 but corresponding boolean dimension is 34190",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-8dd0412b8c51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_pts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 49152 but corresponding boolean dimension is 34190"
     ]
    }
   ],
   "source": [
    "np.arange(len(data_pts))[labels == 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10711,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(data_pts))[label_data == 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10715,)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(len(data_pts))[h5_label == 8].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H5 file make final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "object_dict = {\n",
    "    'BICYCLE' : 4,\n",
    "    'PEDESTRIAN': 1,\n",
    "    'ON_ROAD_OBSTACLE': 5,\n",
    "    'VEHICLE':2,\n",
    "    'LARGE_VEHICLE':3\n",
    "}\n",
    "\n",
    "\n",
    "def compute_dataset( file_paths, h5_batches):\n",
    "    \n",
    "    p = current_process()\n",
    "    print('process counter:', p._identity[0]-1, \" till: \", (p._identity[0]-1)*h5_batches,  'pid:', os.getpid())\n",
    "    process_idx = p._identity[0]\n",
    "    \n",
    "    max_point_num = 8192\n",
    "    batch_size = 2048\n",
    "\n",
    "    data_all= list()\n",
    "    data_num_all= list()\n",
    "    label_all= list()\n",
    "    label_seg_all= list()\n",
    "    indices_all= list()\n",
    "    \n",
    "    id_h5 = (p._identity[0]-1)\n",
    "\n",
    "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
    "\n",
    "        data_idx = (p._identity[0]-1)*h5_batches + idx\n",
    "        \n",
    "        # Get lidar points\n",
    "        actual_index = actual_idx_list[data_idx]\n",
    "        data_points = PyntCloud.from_file(dataset_file)\n",
    "        pts_lidar = data_points\n",
    "\n",
    "        folder = actual_index[0]\n",
    "        dataset = int(actual_index[1])\n",
    "\n",
    "        log, idx = actual_index[0], int(actual_index[1])\n",
    "        lidar_file = dataset_file\n",
    "\n",
    "        argoverse_data = data_loader.get(log)\n",
    "        city_name = argoverse_data.city_name\n",
    "\n",
    "        log_dataidx_list = log_to_count_map[log]\n",
    "        log_data_idx = log_dataidx_list.tolist().index(data_idx)\n",
    "\n",
    "        lidar_pts = argoverse_data.get_lidar(log_data_idx)\n",
    "        city_to_egovehicle_se3 = argoverse_data.get_pose(log_data_idx)\n",
    "        roi_area_pts = city_to_egovehicle_se3.transform_point_cloud(lidar_pts) # more to city CS\n",
    "        roi_area_pts = am.remove_non_roi_points(roi_area_pts, city_name) # remove outside roi points\n",
    "        roi_area_pts = am.remove_ground_surface(roi_area_pts, city_name) # remove ground  \n",
    "        roi_area_pts = city_to_egovehicle_se3.inverse_transform_point_cloud(\n",
    "            roi_area_pts\n",
    "        )# Back to lidar cs\n",
    "\n",
    "        x = np.array(roi_area_pts[:,0])[:, np.newaxis]\n",
    "        y = np.array(roi_area_pts[:,1])[:, np.newaxis]\n",
    "        z = np.array(roi_area_pts[:,2])[:, np.newaxis]\n",
    "        pts_lidar = np.concatenate([x,y,z], axis = 1)\n",
    "\n",
    "        # Get objects\n",
    "        label_file = label_pathlist[data_idx]\n",
    "        objects = get_objects_from_label(label_file)\n",
    "\n",
    "        label_indices = list()\n",
    "        labels = np.zeros(pts_lidar.shape[0])\n",
    "        rgb = np.zeros(pts_lidar.shape[0])\n",
    "        i = np.zeros(pts_lidar.shape[0])\n",
    "\n",
    "        # Get labels\n",
    "        for each in objects:\n",
    "            object_id = object_dict[each.cls_type]\n",
    "\n",
    "            each.pos = np.dot(each.pos, argo_to_kitti)\n",
    "            out = filter_pointcloud(each, np.copy(pts_lidar))\n",
    "\n",
    "            label_indices.extend(out[0].tolist() if type(out[0]) == np.ndarray else out[0])\n",
    "            labels[label_indices] = object_id\n",
    "            rgb[label_indices] = len(out[0])\n",
    "            i[label_indices] = object_id\n",
    "\n",
    "        xyzirgb = pts_lidar\n",
    "\n",
    "        data_full = np.hstack((pts_lidar, rgb[:,np.newaxis]))\n",
    "\n",
    "        label_length, data, data_num, label_seg, indices_split_to_full, item_num, all_label_pred, indices_for_prediction = data_preprocessing(data_full, labels, max_point_num)\n",
    "\n",
    "        data_data =data[0:item_num, ...].astype(np.float32) \n",
    "        data_num =data_num[0:item_num, ...] \n",
    "        indices_split_to_full = indices_split_to_full[0:item_num]\n",
    "        label_seg_data = label_seg[0:item_num]\n",
    "        batch_num = data.shape[0]\n",
    "\n",
    "        data_all.append(data_data)\n",
    "        data_num_all.append(data_num)\n",
    "        label_all.append(item_num*(data_idx+1))\n",
    "        label_seg_all.append(label_seg_data)\n",
    "        indices_all.append(indices_split_to_full)\n",
    "\n",
    "    data_data = np.concatenate(data_all)\n",
    "    data_num_all = np.concatenate(data_num_all)\n",
    "    label_seg_all = np.concatenate(label_seg_all)\n",
    "    indices_all = np.concatenate(indices_all)\n",
    "\n",
    "    # filename_h5 = os.path.join(save_h5_root , folders[0], folders[0]+ '_%s_%d.h5' % (\"full\", data_idx))\n",
    "    folder_name = folders[0][len(os.path.dirname(folders[0]))+1:]\n",
    "    filename_h5 = os.path.join(cwd, \"data\", \"Argo_h5\", folder_name, folder_name+ '_%s_%d.h5' % (\"full\", id_h5))\n",
    "    print(\"Saved to {} \".format(data_idx), filename_h5)\n",
    "\n",
    "    file = h5py.File(filename_h5, 'w')\n",
    "    file.create_dataset('data', data=data_data)\n",
    "    file.create_dataset('data_num', data=data_num_all)\n",
    "    file.create_dataset('label', data=label_all)\n",
    "    file.create_dataset('label_seg', data=label_seg_all)\n",
    "    file.create_dataset('indices_split_to_full', data=indices_all)\n",
    "    file.close()\n",
    "\n",
    "    data_all= list()\n",
    "    data_num_all= list()\n",
    "    label_all= list()\n",
    "    label_seg_all= list()\n",
    "    indices_all= list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total process counter runs:  1312\n",
      "process counter: 46  till:  460 pid: 20681\n",
      "process counter: 51  till:  510 pid: 20686\n",
      "process counter: 50  till:  500 pid: 20685\n",
      "process counter: 48  till:  480 pid: 20683\n",
      "process counter: 49  till:  490 pid: 20684\n",
      "process counter: 53  till:  530 pid: 20688\n",
      "process counter: 57  till:  570 pid: 20692\n",
      "process counter: 56  till:  560 pid: 20691\n",
      "process counter: 47  till:  470 pid: 20682\n",
      "process counter: 52  till:  520 pid: 20687\n",
      "process counter: 60  till:  600 pid: 20695\n",
      "process counter: 55  till:  550 pid: 20690\n",
      "process counter: 61  till:  610 pid: 20696\n",
      "process counter: 62  till:  620 pid: 20697\n",
      "process counter: 63  till:  630 pid: 20698\n",
      "process counter: 64  till:  640 pid: 20699\n",
      "process counter: 58  till:  580 pid: 20693\n",
      "process counter: 65  till:  650 pid: 20700\n",
      "process counter: 59  till:  590 pid: 20694\n",
      "process counter: 54  till:  540 pid: 20689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-56:\n",
      "Process ForkPoolWorker-66:\n",
      "Process ForkPoolWorker-48:\n",
      "Process ForkPoolWorker-51:\n",
      "Process ForkPoolWorker-61:\n",
      "Process ForkPoolWorker-57:\n",
      "Process ForkPoolWorker-59:\n",
      "Process ForkPoolWorker-64:\n",
      "Process ForkPoolWorker-52:\n",
      "Process ForkPoolWorker-49:\n",
      "Process ForkPoolWorker-53:\n",
      "Process ForkPoolWorker-50:\n",
      "Process ForkPoolWorker-47:\n",
      "Process ForkPoolWorker-65:\n",
      "Process ForkPoolWorker-62:\n",
      "Process ForkPoolWorker-63:\n",
      "Process ForkPoolWorker-54:\n",
      "Process ForkPoolWorker-55:\n",
      "Process ForkPoolWorker-58:\n",
      "Process ForkPoolWorker-60:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\", line 47, in starmapstar\n",
      "    return list(itertools.starmap(args[0], args[1]))\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"<ipython-input-352-b07c3c9a9a49>\", line 29, in compute_dataset\n",
      "    for idx, dataset_file in enumerate(tqdm(sorted(file_paths))):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/__init__.py\", line 28, in tqdm_notebook\n",
      "    return _tqdm_notebook(*args, **kwargs)\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 506, in __new__\n",
      "    with cls.get_lock():\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 97, in __enter__\n",
      "    self.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <object repr() failed>\n",
      "Exception ignored in: <object repr() failed>\n",
      "Exception ignored in: <object repr() failed>\n",
      "Exception ignored in: <object repr() failed>\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <object repr() failed>\n",
      "Exception ignored in: <object repr() failed>\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 90, in acquire\n",
      "    lock.acquire()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <object repr() failed>\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <object repr() failed>\n",
      "Exception ignored in: <object repr() failed>\n",
      "Exception ignored in: <object repr() failed>\n",
      "Exception ignored in: <object repr() failed>\n",
      "Exception ignored in: <object repr() failed>\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <object repr() failed>\n",
      "Exception ignored in: <object repr() failed>\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <object repr() failed>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <object repr() failed>\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <object repr() failed>\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "    self.close()\n",
      "    self.close()\n",
      "    self.close()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "Traceback (most recent call last):\n",
      "    self.close()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "    self.close()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "    self.close()\n",
      "    self.close()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "    self.close()\n",
      "    self.close()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/std.py\", line 1039, in __del__\n",
      "    self.close()\n",
      "  File \"/home/kartik/anaconda3/envs/sane35/lib/python3.6/site-packages/tqdm/notebook.py\", line 240, in close\n",
      "    self.close()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-353-1e237e2b5b7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mret_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5_batch_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocess_path_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         '''\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
      "\u001b[0;32m~/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sane35/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sane35/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sane35/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math \n",
    "from multiprocessing import current_process, Pool\n",
    "import multiprocessing\n",
    "\n",
    "object_dict = {\n",
    "    'BICYCLE' : 4,\n",
    "    'PEDESTRIAN': 1,\n",
    "    'ON_ROAD_OBSTACLE': 5,\n",
    "    'VEHICLE':2,\n",
    "    'LARGE_VEHICLE':3\n",
    "}\n",
    "\n",
    "\n",
    "max_point_num = args.max_point_num\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "data_all= list()\n",
    "data_num_all= list()\n",
    "label_all= list()\n",
    "label_seg_all= list()\n",
    "indices_all= list()\n",
    "\n",
    "id_h5 = 0\n",
    "h5_batch_size = 10\n",
    "\n",
    "# create seperate processing pathlists\n",
    "indice = np.arange(lidar_pathlist.__len__())[::h5_batch_size]\n",
    "process_batches = int(lidar_pathlist.__len__()/h5_batch_size)\n",
    "process_path_list = [sorted(lidar_pathlist)[i:i+h5_batch_size] for i in range(process_batches)]\n",
    "\n",
    "print(\"Total process counter runs: \", process_batches)\n",
    "\n",
    "with multiprocessing.Pool(processes=20) as pool:\n",
    "    ret_list = pool.starmap(compute_dataset, [(each_list, h5_batch_size) for each_list in process_path_list])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Old script\n",
    "\n",
    "import math \n",
    "from multiprocessing import current_process, Pool\n",
    "\n",
    "object_dict = {\n",
    "    'BICYCLE' : 4,\n",
    "    'PEDESTRIAN': 1,\n",
    "    'ON_ROAD_OBSTACLE': 5,\n",
    "    'VEHICLE':2,\n",
    "    'LARGE_VEHICLE':3\n",
    "}\n",
    "\n",
    "\n",
    "max_point_num = args.max_point_num\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "data_all= list()\n",
    "data_num_all= list()\n",
    "label_all= list()\n",
    "label_seg_all= list()\n",
    "indices_all= list()\n",
    "\n",
    "id_h5 = 0\n",
    "h5_batches = 5\n",
    "\n",
    "with multiprocessing.Pool(processes=3) as pool:\n",
    "    p.starmap(compute_dataset, [sorted(lidar_pathlist)])\n",
    "    \n",
    "for data_idx, dataset_file in enumerate(tqdm(sorted(lidar_pathlist))):\n",
    "    \n",
    "    # Get lidar points\n",
    "    actual_index = actual_idx_list[data_idx]\n",
    "    data_points = PyntCloud.from_file(dataset_file)\n",
    "    pts_lidar = data_points\n",
    "    \n",
    "    folder = actual_index[0]\n",
    "    dataset = int(actual_index[1])\n",
    "    \n",
    "    log, idx = actual_index[0], int(actual_index[1])\n",
    "    lidar_file = dataset_file\n",
    "\n",
    "    argoverse_data = data_loader.get(log)\n",
    "    city_name = argoverse_data.city_name\n",
    "    \n",
    "    log_dataidx_list = log_to_count_map[log]\n",
    "    log_data_idx = log_dataidx_list.tolist().index(data_idx)\n",
    "    \n",
    "    lidar_pts = argoverse_data.get_lidar(log_data_idx)\n",
    "    city_to_egovehicle_se3 = argoverse_data.get_pose(log_data_idx)\n",
    "    roi_area_pts = city_to_egovehicle_se3.transform_point_cloud(lidar_pts) # more to city CS\n",
    "    roi_area_pts = am.remove_non_roi_points(roi_area_pts, city_name) # remove outside roi points\n",
    "    roi_area_pts = am.remove_ground_surface(roi_area_pts, city_name) # remove ground  \n",
    "    roi_area_pts = city_to_egovehicle_se3.inverse_transform_point_cloud(\n",
    "        roi_area_pts\n",
    "    )# Back to lidar cs\n",
    "\n",
    "    x = np.array(roi_area_pts[:,0])[:, np.newaxis]\n",
    "    y = np.array(roi_area_pts[:,1])[:, np.newaxis]\n",
    "    z = np.array(roi_area_pts[:,2])[:, np.newaxis]\n",
    "    pts_lidar = np.concatenate([x,y,z], axis = 1)\n",
    "    \n",
    "    # Get objects\n",
    "    label_file = label_pathlist[data_idx]\n",
    "    objects = get_objects_from_label(label_file)\n",
    "    \n",
    "    label_indices = list()\n",
    "    labels = np.zeros(pts_lidar.shape[0])\n",
    "    rgb = np.zeros(pts_lidar.shape[0])\n",
    "    i = np.zeros(pts_lidar.shape[0])\n",
    "    \n",
    "    # Get labels\n",
    "    for each in objects:\n",
    "        object_id = object_dict[each.cls_type]\n",
    "        \n",
    "        each.pos = np.dot(each.pos, argo_to_kitti)\n",
    "        out = filter_pointcloud(each, np.copy(pts_lidar))\n",
    "        \n",
    "        label_indices.extend(out[0].tolist() if type(out[0]) == np.ndarray else out[0])\n",
    "        labels[label_indices] = object_id\n",
    "        rgb[label_indices] = len(out[0])\n",
    "        i[label_indices] = object_id\n",
    "\n",
    "    xyzirgb = pts_lidar\n",
    "    \n",
    "    data_full = np.hstack((pts_lidar, rgb[:,np.newaxis]))\n",
    "    \n",
    "    label_length, data, data_num, label_seg, indices_split_to_full, item_num, all_label_pred, indices_for_prediction = data_preprocessing(data_full, labels, max_point_num)\n",
    "\n",
    "    data_data =data[0:item_num, ...].astype(np.float32) \n",
    "    data_num =data_num[0:item_num, ...] \n",
    "    indices_split_to_full = indices_split_to_full[0:item_num]\n",
    "    label_seg_data = label_seg[0:item_num]\n",
    "    batch_num = data.shape[0]\n",
    "    \n",
    "    data_all.append(data_data)\n",
    "    data_num_all.append(data_num)\n",
    "    label_all.append(item_num*(data_idx+1))\n",
    "    label_seg_all.append(label_seg_data)\n",
    "    indices_all.append(indices_split_to_full)\n",
    "\n",
    "    if data_idx%h5_batches == 0 and data_idx != 0:\n",
    "\n",
    "        data_data = np.concatenate(data_all)\n",
    "        data_num_all = np.concatenate(data_num_all)\n",
    "        label_seg_all = np.concatenate(label_seg_all)\n",
    "        indices_all = np.concatenate(indices_all)\n",
    "\n",
    "        # filename_h5 = os.path.join(save_h5_root , folders[0], folders[0]+ '_%s_%d.h5' % (\"full\", data_idx))\n",
    "        folder_name = folders[0][len(os.path.dirname(folders[0]))+1:]\n",
    "        filename_h5 = os.path.join(cwd, \"data\", \"Argo_h5\", folder_name, folder_name+ '_%s_%d.h5' % (\"full\", id_h5))\n",
    "        print(\"Saved to {} \".format(data_idx), filename_h5)\n",
    "\n",
    "        file = h5py.File(filename_h5, 'w')\n",
    "        file.create_dataset('data', data=data_data)\n",
    "        file.create_dataset('data_num', data=data_num_all)\n",
    "        file.create_dataset('label', data=label_all)\n",
    "        file.create_dataset('label_seg', data=label_seg_all)\n",
    "        file.create_dataset('indices_split_to_full', data=indices_all)\n",
    "        file.close()\n",
    "        \n",
    "        data_all= list()\n",
    "        data_num_all= list()\n",
    "        label_all= list()\n",
    "        label_seg_all= list()\n",
    "        indices_all= list()\n",
    "        \n",
    "        id_h5+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 8192, 3)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(data_all).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f266e778d516462ea0a7269decf3e512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_keys = list()\n",
    "for data_idx, dataset_file in enumerate(tqdm(sorted(lidar_pathlist))):\n",
    "    objects = get_objects_from_label(label_file)\n",
    "    for each in objects:\n",
    "        all_keys.append(each.cls_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LARGE_VEHICLE', 'ON_ROAD_OBSTACLE', 'PEDESTRIAN', 'VEHICLE'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(all_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['data', 'data_num', 'indices_split_to_full', 'label', 'label_seg']>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_test = os.path.join(cwd, \"data\", \"Argo_h5\", \"train\", \"train_full_19.h5\")\n",
    "data = h5py.File(file_test)\n",
    "data.keys()\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2610,), (2610, 8192, 3), (2610, 8192), (250,), (2610, 8192))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data_num'].shape, data['data'].shape, data['indices_split_to_full'].shape, data['label'].shape, data['label_seg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "labels = []\n",
    "point_nums = []\n",
    "labels_seg = []\n",
    "indices_split_to_full = []\n",
    "\n",
    "points.append(data['data'][:,:,0:4].astype(np.float32))\n",
    "labels.append(data['label'][...].astype(np.int64))\n",
    "point_nums.append(data['data_num'][...].astype(np.int32))\n",
    "labels_seg.append(data['label_seg'][...].astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7066408169f4dc0b940796d2e483257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, position=(4.580923080444336, 35.17377305030823, 171.79…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c51d416fca4189899a95de9a42d07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.02, max=0.2, step=0.0002), Label(value='Backgro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_pts = np.concatenate(points[0][0:6, :, :3])\n",
    "label_idx = np.concatenate(labels_seg[0][0:6, :])\n",
    "new_pts1 = pd.DataFrame(np.array(data_pts)[:, 0:3], columns=['x', 'y', 'z'])\n",
    "new_pts1[\"red\"]  = 255\n",
    "new_pts1[\"green\"]  = 121\n",
    "new_pts1[\"blue\"]  = 121\n",
    "labelled_pts = data_pts[label_idx == 8]\n",
    "new_pts2 = pd.DataFrame(np.array(labelled_pts)[:, 0:3], columns=['x', 'y', 'z'])\n",
    "new_pts2[\"red\"]  = 255\n",
    "new_pts2[\"green\"]  = 255\n",
    "new_pts2[\"blue\"]  = 255\n",
    "cloud = PyntCloud(new_pts1.append(new_pts2))\n",
    "cloud.plot(initial_point_size=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((409600, 3), (409600, 3))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_pts.shape, data_pts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-23 01:31:47.127580-Preparing datasets...\n",
      "not h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:argoverse.data_loading.synchronization_database:No corresponding stereo image at 315967661819717000: 176.11196 > 100.0 ms\n",
      "WARNING:argoverse.data_loading.synchronization_database:No corresponding stereo image at 315967661819717000: 176.107928 > 100.0 ms\n",
      "WARNING:argoverse.data_loading.synchronization_database:No corresponding stereo image at 315967953120029000: 170.267688 > 100.0 ms\n",
      "WARNING:argoverse.data_loading.synchronization_database:No corresponding stereo image at 315967953120029000: 170.390336 > 100.0 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image list sample is:  ['3d20ae25-5b29-320d-8bae-f03e9dc177b9', '315975023020283000']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'data_utils' has no attribute 'load_seg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-97afa17ff1c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image list sample is: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactual_idx_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdata_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_num_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_seg2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilelist_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;31m# data_val, _, data_num_val, label_val, _ = data_utils.load_seg(args.filelist_val)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'data_utils' has no attribute 'load_seg2'"
     ]
    }
   ],
   "source": [
    "# Prepare inputs\n",
    "print('{}-Preparing datasets...'.format(datetime.now()))\n",
    "is_list_of_h5_list = not data_utils.is_h5_list(args['filelist'])\n",
    "\n",
    "print(\"not h5\")\n",
    "filelist_train = args['filelist']\n",
    "    \n",
    "lidar_pathlist = list()\n",
    "label_pathlist = list()\n",
    "\n",
    "root_dir = os.path.join(cwd, \"data\", \"Argo\")\n",
    "# root_dir = os.path.join(\"/home/safeai-gpu2/mnt/safeai-dataserver/argoverse-1.1/argoverse-tracking\")\n",
    "split = \"train\"\n",
    "data_loader = ArgoverseTrackingLoader(os.path.join(root_dir,split))\n",
    "log_list = data_loader.log_list\n",
    "for log in log_list:\n",
    "    lidar_pathlist.extend(data_loader.get(log).lidar_list)\n",
    "    label_pathlist.extend(data_loader.get(log).label_list)\n",
    "assert len(lidar_pathlist) == len(label_pathlist)\n",
    "# am = ArgoverseMap()\n",
    "calib_file = data_loader.calib_filename\n",
    "num_sample = len(lidar_pathlist)\n",
    "image_idx_list = np.arange(num_sample)\n",
    "imageset_dir = os.path.join(root_dir,split)\n",
    "splitname = lambda x: [x[len(imageset_dir+\"/\"):-4].split(\"/\")[0], x[len(imageset_dir+\"/\"):-4].split(\"/\")[2].split(\"_\")[1]]\n",
    "actual_idx_list = [splitname(each) for each in lidar_pathlist]\n",
    "print(\"image list sample is: \", actual_idx_list[0])\n",
    "\n",
    "data_train, _, data_num_train, label_train, _ = data_utils.load_seg2(filelist_train)\n",
    "# data_val, _, data_num_val, label_val, _ = data_utils.load_seg(args.filelist_val)\n",
    "\n",
    "# shuffle\n",
    "data_train, data_num_train, label_train = \\\n",
    "    data_utils.grouped_shuffle([data_train, data_num_train, label_train])\n",
    "\n",
    "num_train = data_train.shape[0]\n",
    "point_num = data_train.shape[1]\n",
    "num_val = data_train.shape[0]\n",
    "print('{}-{:d}/{:d} training/validation samples.'.format(datetime.now(), num_train, num_val))\n",
    "batch_num = (num_train * num_epochs + batch_size - 1) // batch_size\n",
    "print('{}-{:d} training batches.'.format(datetime.now(), batch_num))\n",
    "batch_num_val = math.ceil(num_val / batch_size)\n",
    "print('{}-{:d} testing batches per test.'.format(datetime.now(), batch_num_val))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kartik/DL_model/PointCNN/data/KITTI/ImageSets/train.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9bc603f778451182ab0bf3334543aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=51), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-18569e879d3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lidar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlidar_pathlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_pathlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "points = []\n",
    "labels = []\n",
    "point_nums = []\n",
    "labels_seg = []\n",
    "indices_split_to_full = []\n",
    "\n",
    "folder = os.path.dirname(filelist_train)\n",
    "print(filelist_train)\n",
    "file = open(filelist_train)\n",
    "\n",
    "for line_num in tqdm(range(len(filelist))):\n",
    "    line = next(file)\n",
    "    idx  = line[:-2].split(\",\")\n",
    "    am = 0\n",
    "    data = get_lidar(data_loader, am, lidar_pathlist, idx, line_num)\n",
    "    points.append(data.astype(np.float32))\n",
    "\n",
    "    label = get_label(label_pathlist, line_num)\n",
    "    \n",
    "    labels.append(data['label'][...].astype(np.int64))\n",
    "    point_nums.append(data['data_num'][...].astype(np.int32))\n",
    "    labels_seg.append(data['label_seg'][...].astype(np.int64))\n",
    "    if 'indices_split_to_full' in data:\n",
    "        indices_split_to_full.append(data['indices_split_to_full'][...].astype(np.int64))\n",
    "file.close()\n",
    "_point = np.concatenate(points, axis=0)\n",
    "\n",
    "_ref_max = np.max(_point[:,:,3:4].flatten())\n",
    "_dens_max = np.max(_point[:,:,4:5].flatten())\n",
    "\n",
    "_point[:,:,3:4] = (  _point[:,:,3:4] / _ref_max ) - 0.5\n",
    "_point[:,:,4:5] = (  _point[:,:,4:5] / _dens_max ) - 0.5\n",
    "\n",
    "print(\"amax\", np.amax(_point, axis=0, keepdims=True))\n",
    "print(\"amin\", np.amin(_point, axis=0, keepdims=True))\n",
    "\n",
    "return (_point[:,:,[0,1,2,3, 4]],\n",
    "        np.concatenate(labels, axis=0),\n",
    "        np.concatenate(point_nums, axis=0),\n",
    "        np.concatenate(labels_seg, axis=0),\n",
    "        np.concatenate(indices_split_to_full, axis=0) if indices_split_to_full else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(label_pathlist, idx):\n",
    "\n",
    "    label_file = label_pathlist[idx]\n",
    "    assert os.path.exists(label_file)\n",
    "\n",
    "    return kitti_utils.get_objects_from_label(label_file)\n",
    "\n",
    "def get_objects_from_label(label_file):\n",
    "    # Opens a label file, and passes the object to Object3d object, Read the json GT labels\n",
    "    \n",
    "    f = open(label_file)\n",
    "    label_data = json.load(f) \n",
    "    objects = [object3d.Object3d(data) for data in label_data]\n",
    "    return objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizulaization helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202fa97a19e248879f1408a465c841a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(aspect=1.6, fov=90.0, quaternion=(0.0, 0.0, 0.0, 1.0), scale=(1.0, 1.0, 1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9936e0962c40cea738991ecd13c365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Label(value='Point size:'), FloatSlider(value=0.02, max=0.2, step=0.0002), Label(value='Backgro…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_pts = pd.DataFrame(np.array(np.concatenate(data[7:10, :, :3], axis=0))[:, 0:3], columns=['x', 'y', 'z'])\n",
    "cloud = PyntCloud(new_pts)\n",
    "cloud.plot(initial_point_size=0.02)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
