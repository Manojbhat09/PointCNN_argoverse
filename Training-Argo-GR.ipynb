{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import argparse\n",
    "import importlib\n",
    "import data_utils_legacy as data_utils\n",
    "import numpy as np\n",
    "import pointfly as pf\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, epochs=1, filelist='data/KITTI/ImageSets/train.txt', filelist_val='data/KITTI/ImageSets/train.txt', load_ckpt=None, log='log.txt', model='pointcnn_seg', no_code_backup=False, no_timestamp_folder=False, save_folder='/home/kartik/saved/', setting='kitti3d_x8_2048_fps')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--filelist', '-t', default=\"data/KITTI/ImageSets/train.txt\", help='Path to training set ground truth (.txt)')\n",
    "parser.add_argument('--filelist_val', '-v', default=\"data/KITTI/ImageSets/train.txt\", help='Path to validation set ground truth (.txt)')\n",
    "parser.add_argument('--load_ckpt', '-l', help='Path to a check point file for load')\n",
    "parser.add_argument('--save_folder', '-s', default=\"/home/kartik/saved/\", help='Path to folder for saving check points and summary')\n",
    "parser.add_argument('--model', '-m', default=\"pointcnn_seg\", help='Model to use')\n",
    "parser.add_argument('--setting', '-x', default=\"kitti3d_x8_2048_fps\", help='Setting to use')\n",
    "parser.add_argument('--epochs', default=\"1\",help='Number of training epochs (default defined in setting)', type=int)\n",
    "parser.add_argument('--batch_size', default=\"1\", help='Batch size (default defined in setting)', type=int)\n",
    "# default=\"64\",\n",
    "parser.add_argument('--log', help='Log to FILE in save folder; use - for stdout (default is log.txt)', metavar='FILE', default='log.txt')\n",
    "parser.add_argument('--no_timestamp_folder', help='Dont save to timestamp folder', action='store_true')\n",
    "parser.add_argument('--no_code_backup', help='Dont backup code', action='store_true')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "model = importlib.import_module(args.model)\n",
    "setting_path = os.path.join(cwd, args.model)\n",
    "sys.path.append(setting_path)\n",
    "setting = importlib.import_module(args.setting)\n",
    "\n",
    "num_epochs = args.epochs or setting.num_epochs\n",
    "batch_size = args.batch_size or setting.batch_size\n",
    "sample_num = setting.sample_num\n",
    "step_val = setting.step_val\n",
    "label_weights_list = setting.label_weights\n",
    "rotation_range = setting.rotation_range\n",
    "rotation_range_val = setting.rotation_range_val\n",
    "scaling_range = setting.scaling_range\n",
    "scaling_range_val = setting.scaling_range_val\n",
    "jitter = setting.jitter\n",
    "jitter_val = setting.jitter_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-06 07:45:40.138612-Preparing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:08,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amax [[[217.02274   84.14794  194.86005 ]\n",
      "  [216.58429   59.07181  190.29214 ]\n",
      "  [218.70334   82.557396 199.96799 ]\n",
      "  ...\n",
      "  [143.4347    13.510128  61.26613 ]\n",
      "  [131.37718   10.249168 111.17418 ]\n",
      "  [ 53.269745   7.663868 111.337364]]]\n",
      "amin [[[-213.55579      -7.4701376  -212.08127   ]\n",
      "  [-212.70149      -3.2587485  -211.15894   ]\n",
      "  [-215.07878      -5.7737727  -209.11053   ]\n",
      "  ...\n",
      "  [ -74.75722      -0.47247314  -64.9187    ]\n",
      "  [ -98.06576      -0.66090035 -173.12178   ]\n",
      "  [ -82.89937      -0.5216615   -47.190056  ]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amax [[[215.90323   20.952639  94.15194 ]\n",
      "  [155.82857   18.881943  94.48389 ]\n",
      "  [162.71529   21.812824  94.183495]\n",
      "  ...\n",
      "  [  0.         0.         0.      ]\n",
      "  [  0.         0.         0.      ]\n",
      "  [  0.         0.         0.      ]]]\n",
      "amin [[[-140.0768       -0.49999964 -119.15227   ]\n",
      "  [-192.31703      -1.0448203  -117.7571    ]\n",
      "  [-162.12273      -0.57638097 -123.3467    ]\n",
      "  ...\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]\n",
      "  [   0.            0.            0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "points = []\n",
    "labels = []\n",
    "point_nums = []\n",
    "labels_seg = []\n",
    "indices_split_to_full = []\n",
    "\n",
    "print('{}-Preparing datasets...'.format(datetime.now()))\n",
    "# is_list_of_h5_list = data_utils.is_h5_list(args.filelist)\n",
    "args.filelist = os.path.join(cwd, \"data\", \"Argo_h5_GR_scaled_depth\", \"train\")+\"/train.txt\"\n",
    "args.filelist_val = os.path.join(cwd, \"data\", \"Argo_h5_GR_scaled_depth\", \"train\")+\"/val.txt\"\n",
    "\n",
    "is_list_of_h5_list = not data_utils.is_h5_list(args.filelist)\n",
    "if is_list_of_h5_list:\n",
    "    seg_list = data_utils.load_seg_list(args.filelist)\n",
    "    seg_list_idx = 0\n",
    "    filelist_train = seg_list[seg_list_idx]\n",
    "    seg_list_idx = seg_list_idx + 1\n",
    "else:\n",
    "    filelist_train = args.filelist\n",
    "data_train, _, data_num_train, label_train, _ = data_utils.load_seg_Argo(filelist_train)\n",
    "data_val, _, data_num_val, label_val, _ = data_utils.load_seg_Argo(args.filelist_val)\n",
    "\n",
    "\n",
    "# data_train = np.dstack([data_train, label_train[:, :,np.newaxis]])\n",
    "# data_val = np.dstack([data_val, label_val[:, :,np.newaxis]])\n",
    "\n",
    "depth = np.linalg.norm(data_train, 2, axis=2)\n",
    "data_train = np.concatenate([data_train, depth[:,:,np.newaxis]], axis = -1)\n",
    "\n",
    "depth = np.linalg.norm(data_val, 2, axis=2)\n",
    "data_val = np.concatenate([data_val, depth[:,:,np.newaxis]], axis = -1)\n",
    "\n",
    "# shuffle\n",
    "data_train, data_num_train, label_train = \\\n",
    "    data_utils.grouped_shuffle([data_train, data_num_train, label_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-06 07:46:07.197767-43942/1639 training/validation samples.\n",
      "2020-01-06 07:46:07.198041-43942 training batches.\n",
      "2020-01-06 07:46:07.198114-1639 testing batches per test.\n"
     ]
    }
   ],
   "source": [
    "num_train = data_train.shape[0]\n",
    "point_num = data_train.shape[1]\n",
    "num_val = data_val.shape[0]\n",
    "print('{}-{:d}/{:d} training/validation samples.'.format(datetime.now(), num_train, num_val))\n",
    "batch_num = (num_train * num_epochs + batch_size - 1) // batch_size\n",
    "print('{}-{:d} training batches.'.format(datetime.now(), batch_num))\n",
    "batch_num_val = math.ceil(num_val / batch_size)\n",
    "print('{}-{:d} testing batches per test.'.format(datetime.now(), batch_num_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Placeholders\n",
    "indices = tf.placeholder(tf.int32, shape=(None, None, 2), name=\"indices\")\n",
    "xforms = tf.placeholder(tf.float32, shape=(None, 3, 3), name=\"xforms\")\n",
    "rotations = tf.placeholder(tf.float32, shape=(None, 3, 3), name=\"rotations\")\n",
    "jitter_range = tf.placeholder(tf.float32, shape=(1), name=\"jitter_range\")\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "pts_fts = tf.placeholder(tf.float32, shape=(None, point_num, setting.data_dim), name='pts_fts')\n",
    "labels_seg = tf.placeholder(tf.int64, shape=(None, point_num), name='labels_seg')\n",
    "labels_weights = tf.placeholder(tf.float32, shape=(None, point_num), name='labels_weights')\n",
    "\n",
    "######################################################################\n",
    "pts_fts_sampled = tf.gather_nd(pts_fts, indices=indices, name='pts_fts_sampled')\n",
    "features_augmented = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setting.data_dim > 3:\n",
    "    points_sampled, features_sampled = tf.split(pts_fts_sampled,\n",
    "                                                [3, setting.data_dim - 3],\n",
    "                                                axis=-1,\n",
    "                                                name='split_points_features')\n",
    "    if setting.use_extra_features:\n",
    "        if setting.with_normal_feature:\n",
    "            if setting.data_dim < 6:\n",
    "                print('Only 3D normals are supported!')\n",
    "                exit()\n",
    "            elif setting.data_dim == 6:\n",
    "                features_augmented = pf.augment(features_sampled, rotations)\n",
    "            else:\n",
    "                normals, rest = tf.split(features_sampled, [3, setting.data_dim - 6])\n",
    "                normals_augmented = pf.augment(normals, rotations)\n",
    "                features_augmented = tf.concat([normals_augmented, rest], axis=-1)\n",
    "        else:\n",
    "            features_augmented = features_sampled\n",
    "else:\n",
    "    points_sampled = pts_fts_sampled\n",
    "points_augmented = pf.augment(points_sampled, xforms, jitter_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling path  /home/kartik/DL_model/PointCNN/sampling\n",
      "WARNING:tensorflow:From /home/kartik/DL_model/PointCNN/pointfly.py:193: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "labels_sampled = tf.gather_nd(labels_seg, indices=indices, name='labels_sampled')\n",
    "labels_weights_sampled = tf.gather_nd(labels_weights, indices=indices, name='labels_weight_sampled')\n",
    "\n",
    "net = model.Net(points_augmented, features_augmented, is_training, setting)\n",
    "logits = net.logits\n",
    "probs = tf.nn.softmax(logits, name='probs')\n",
    "predictions = tf.argmax(probs, axis=-1, name='predictions')\n",
    "\n",
    "loss_op = tf.losses.sparse_softmax_cross_entropy(labels=labels_sampled, logits=logits,\n",
    "                                                 weights=labels_weights_sampled)\n",
    "\n",
    "with tf.name_scope('metrics'):\n",
    "    loss_mean_op, loss_mean_update_op = tf.metrics.mean(loss_op)\n",
    "    t_1_acc_op, t_1_acc_update_op = tf.metrics.accuracy(labels_sampled, predictions, weights=labels_weights_sampled)\n",
    "    t_1_per_class_acc_op, t_1_per_class_acc_update_op = \\\n",
    "        tf.metrics.mean_per_class_accuracy(labels_sampled, predictions, setting.num_class,\n",
    "                                           weights=labels_weights_sampled)\n",
    "reset_metrics_op = tf.variables_initializer([var for var in tf.local_variables()\n",
    "                                             if var.name.split('/')[0] == 'metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tf.summary.scalar('loss/train', tensor=loss_mean_op, collections=['train'])\n",
    "_ = tf.summary.scalar('t_1_acc/train', tensor=t_1_acc_op, collections=['train'])\n",
    "_ = tf.summary.scalar('t_1_per_class_acc/train', tensor=t_1_per_class_acc_op, collections=['train'])\n",
    "\n",
    "_ = tf.summary.scalar('loss/val', tensor=loss_mean_op, collections=['val'])\n",
    "_ = tf.summary.scalar('t_1_acc/val', tensor=t_1_acc_op, collections=['val'])\n",
    "_ = tf.summary.scalar('t_1_per_class_acc/val', tensor=t_1_per_class_acc_op, collections=['val'])\n",
    "\n",
    "lr_exp_op = tf.train.exponential_decay(setting.learning_rate_base, global_step, setting.decay_steps,\n",
    "                                       setting.decay_rate, staircase=True)\n",
    "lr_clip_op = tf.maximum(lr_exp_op, setting.learning_rate_min)\n",
    "_ = tf.summary.scalar('learning_rate', tensor=lr_clip_op, collections=['train'])\n",
    "reg_loss = setting.weight_decay * tf.losses.get_regularization_loss()\n",
    "if setting.optimizer == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr_clip_op, epsilon=setting.epsilon)\n",
    "elif setting.optimizer == 'momentum':\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=lr_clip_op, momentum=setting.momentum, use_nesterov=True)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss_op + reg_loss, global_step=global_step)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-06 07:46:20.442628-Parameter number: 11510276.\n"
     ]
    }
   ],
   "source": [
    "# backup all code\n",
    "# if not args.no_code_backup:\n",
    "#     code_folder = os.path.abspath(os.path.dirname(__file__))\n",
    "#     shutil.copytree(code_folder, os.path.join(root_folder, os.path.basename(code_folder)))\n",
    "\n",
    "root_folder = os.path.join(cwd, \"outfolder_argo_gr3\")\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "# shutil.copytree(code_folder, os.path.join(root_folder, os.path.basename(code_folder)))\n",
    "\n",
    "folder_ckpt = os.path.join(root_folder, 'ckpts')\n",
    "if not os.path.exists(folder_ckpt):\n",
    "    os.makedirs(folder_ckpt)\n",
    "\n",
    "folder_summary = os.path.join(root_folder, 'summary')\n",
    "if not os.path.exists(folder_summary):\n",
    "    os.makedirs(folder_summary)\n",
    "\n",
    "parameter_num = np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])\n",
    "print('{}-Parameter number: {:d}.'.format(datetime.now(), parameter_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-06 07:46:25.946712-Found checkpoint /home/kartik/DL_model/PointCNN/outfolder_argo_gr3/ckpts/iter-98004\n",
      "INFO:tensorflow:Restoring parameters from /home/kartik/DL_model/PointCNN/outfolder_argo_gr3/ckpts/iter-98004\n",
      "2020-01-06 07:46:27.432956-Checkpoint loaded from /home/kartik/DL_model/PointCNN/outfolder_argo_gr3/ckpts/iter-98004 (Iter 98004)\n",
      "2020-01-06 07:46:40.253320-[Train]-Iter: 098005  Loss: 0.0700  T-1 Acc: 0.9032  T-1 mAcc: 0.4204\n",
      "INFO:tensorflow:/home/kartik/DL_model/PointCNN/outfolder_argo_gr3/ckpts/iter-98005 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "2020-01-06 07:46:41.536004-Checkpoint saved to /home/kartik/DL_model/PointCNN/outfolder_argo_gr3/ckpts/iter!\n",
      "2020-01-06 07:46:45.506376-[Train]-Iter: 098015  Loss: 0.0289  T-1 Acc: 0.9539  T-1 mAcc: 0.4754\n",
      "2020-01-06 07:46:49.524854-[Train]-Iter: 098025  Loss: 0.1043  T-1 Acc: 0.8670  T-1 mAcc: 0.4324\n",
      "2020-01-06 07:46:53.537828-[Train]-Iter: 098035  Loss: 0.0242  T-1 Acc: 0.9674  T-1 mAcc: 0.4804\n",
      "2020-01-06 07:46:57.557198-[Train]-Iter: 098045  Loss: 0.1108  T-1 Acc: 0.8957  T-1 mAcc: 0.4544\n",
      "2020-01-06 07:47:01.564888-[Train]-Iter: 098055  Loss: 0.0528  T-1 Acc: 0.7509  T-1 mAcc: 0.3290\n",
      "2020-01-06 07:47:05.568501-[Train]-Iter: 098065  Loss: 0.0407  T-1 Acc: 0.9681  T-1 mAcc: 0.4681\n",
      "2020-01-06 07:47:09.571065-[Train]-Iter: 098075  Loss: 0.0872  T-1 Acc: 0.9610  T-1 mAcc: 0.4821\n",
      "2020-01-06 07:47:13.611351-[Train]-Iter: 098085  Loss: 0.0353  T-1 Acc: 0.9649  T-1 mAcc: 0.4745\n",
      "2020-01-06 07:47:17.650055-[Train]-Iter: 098095  Loss: 0.0098  T-1 Acc: 0.9944  T-1 mAcc: 0.4972\n",
      "2020-01-06 07:47:21.663893-[Train]-Iter: 098105  Loss: 0.0468  T-1 Acc: 0.9454  T-1 mAcc: 0.4711\n",
      "2020-01-06 07:47:25.642091-[Train]-Iter: 098115  Loss: 0.0569  T-1 Acc: 0.8927  T-1 mAcc: 0.4461\n",
      "2020-01-06 07:47:29.600472-[Train]-Iter: 098125  Loss: 0.0244  T-1 Acc: 0.9793  T-1 mAcc: 0.4869\n",
      "2020-01-06 07:47:33.461752-[Train]-Iter: 098135  Loss: 0.1172  T-1 Acc: 0.7038  T-1 mAcc: 0.3118\n",
      "2020-01-06 07:47:37.475879-[Train]-Iter: 098145  Loss: 0.0372  T-1 Acc: 0.9705  T-1 mAcc: 0.4786\n",
      "2020-01-06 07:47:41.447070-[Train]-Iter: 098155  Loss: 0.0838  T-1 Acc: 0.8643  T-1 mAcc: 0.4214\n",
      "2020-01-06 07:47:45.415003-[Train]-Iter: 098165  Loss: 0.0292  T-1 Acc: 0.8735  T-1 mAcc: 0.2184\n",
      "2020-01-06 07:47:49.411219-[Train]-Iter: 098175  Loss: 0.0733  T-1 Acc: 0.9203  T-1 mAcc: 0.4653\n",
      "2020-01-06 07:47:53.335051-[Train]-Iter: 098185  Loss: 0.0552  T-1 Acc: 0.8525  T-1 mAcc: 0.2131\n",
      "2020-01-06 07:47:57.346505-[Train]-Iter: 098195  Loss: 0.1113  T-1 Acc: 0.9026  T-1 mAcc: 0.4541\n",
      "2020-01-06 07:48:01.298734-[Train]-Iter: 098205  Loss: 0.0954  T-1 Acc: 0.9423  T-1 mAcc: 0.4556\n",
      "2020-01-06 07:48:05.245373-[Train]-Iter: 098215  Loss: 0.0135  T-1 Acc: 0.9904  T-1 mAcc: 0.4920\n",
      "2020-01-06 07:48:07.885613-[Train]-Iter: 098225  Loss: 0.1552  T-1 Acc: 0.8728  T-1 mAcc: 0.4155\n",
      "2020-01-06 07:48:09.851467-[Train]-Iter: 098235  Loss: 0.0246  T-1 Acc: 0.9523  T-1 mAcc: 0.4617\n",
      "2020-01-06 07:48:11.900393-[Train]-Iter: 098245  Loss: 0.0645  T-1 Acc: 0.8968  T-1 mAcc: 0.4394\n",
      "2020-01-06 07:48:13.973549-[Train]-Iter: 098255  Loss: 0.0004  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2020-01-06 07:48:16.037232-[Train]-Iter: 098265  Loss: 0.0135  T-1 Acc: 0.9595  T-1 mAcc: 0.2399\n",
      "2020-01-06 07:48:18.099933-[Train]-Iter: 098275  Loss: 0.0655  T-1 Acc: 0.7183  T-1 mAcc: 0.1796\n",
      "2020-01-06 07:48:20.201851-[Train]-Iter: 098285  Loss: 0.0374  T-1 Acc: 0.8164  T-1 mAcc: 0.2041\n",
      "2020-01-06 07:48:22.247538-[Train]-Iter: 098295  Loss: 0.0340  T-1 Acc: 0.9736  T-1 mAcc: 0.4860\n",
      "2020-01-06 07:48:24.324927-[Train]-Iter: 098305  Loss: 0.0731  T-1 Acc: 0.9477  T-1 mAcc: 0.4773\n",
      "2020-01-06 07:48:26.405056-[Train]-Iter: 098315  Loss: 0.0507  T-1 Acc: 0.9676  T-1 mAcc: 0.4759\n",
      "2020-01-06 07:48:28.453646-[Train]-Iter: 098325  Loss: 0.0471  T-1 Acc: 0.9427  T-1 mAcc: 0.4613\n",
      "2020-01-06 07:48:30.526335-[Train]-Iter: 098335  Loss: 0.0326  T-1 Acc: 0.9631  T-1 mAcc: 0.4813\n",
      "2020-01-06 07:48:32.581639-[Train]-Iter: 098345  Loss: 0.0612  T-1 Acc: 0.7427  T-1 mAcc: 0.1857\n",
      "2020-01-06 07:48:34.645644-[Train]-Iter: 098355  Loss: 0.0624  T-1 Acc: 0.9405  T-1 mAcc: 0.4684\n",
      "2020-01-06 07:48:36.713501-[Train]-Iter: 098365  Loss: 0.3239  T-1 Acc: 0.2807  T-1 mAcc: 0.2244\n",
      "2020-01-06 07:48:38.785316-[Train]-Iter: 098375  Loss: 0.0724  T-1 Acc: 0.7710  T-1 mAcc: 0.1927\n",
      "2020-01-06 07:48:40.822651-[Train]-Iter: 098385  Loss: 0.0810  T-1 Acc: 0.9441  T-1 mAcc: 0.4486\n",
      "2020-01-06 07:48:42.878561-[Train]-Iter: 098395  Loss: 0.1832  T-1 Acc: 0.8237  T-1 mAcc: 0.4331\n",
      "2020-01-06 07:48:44.942312-[Train]-Iter: 098405  Loss: 0.0108  T-1 Acc: 0.9590  T-1 mAcc: 0.2397\n",
      "2020-01-06 07:48:46.999899-[Train]-Iter: 098415  Loss: 0.0503  T-1 Acc: 0.9407  T-1 mAcc: 0.4740\n",
      "2020-01-06 07:48:49.069007-[Train]-Iter: 098425  Loss: 0.0574  T-1 Acc: 0.9493  T-1 mAcc: 0.4539\n",
      "2020-01-06 07:48:51.128838-[Train]-Iter: 098435  Loss: 0.0698  T-1 Acc: 0.9583  T-1 mAcc: 0.4222\n",
      "2020-01-06 07:48:53.184562-[Train]-Iter: 098445  Loss: 0.0116  T-1 Acc: 0.9824  T-1 mAcc: 0.4891\n",
      "2020-01-06 07:48:55.244965-[Train]-Iter: 098455  Loss: 0.0348  T-1 Acc: 0.9636  T-1 mAcc: 0.4726\n",
      "2020-01-06 07:48:57.295764-[Train]-Iter: 098465  Loss: 0.1161  T-1 Acc: 0.9401  T-1 mAcc: 0.4699\n",
      "2020-01-06 07:48:59.360252-[Train]-Iter: 098475  Loss: 0.0114  T-1 Acc: 0.9758  T-1 mAcc: 0.4880\n",
      "2020-01-06 07:49:01.428713-[Train]-Iter: 098485  Loss: 0.0599  T-1 Acc: 0.8242  T-1 mAcc: 0.2061\n",
      "2020-01-06 07:49:03.503361-[Train]-Iter: 098495  Loss: 0.0402  T-1 Acc: 0.9766  T-1 mAcc: 0.4865\n",
      "2020-01-06 07:49:05.563339-[Train]-Iter: 098505  Loss: 0.0596  T-1 Acc: 0.8405  T-1 mAcc: 0.4153\n",
      "2020-01-06 07:49:07.612685-[Train]-Iter: 098515  Loss: 0.0185  T-1 Acc: 0.9835  T-1 mAcc: 0.4898\n",
      "2020-01-06 07:49:09.664594-[Train]-Iter: 098525  Loss: 0.2622  T-1 Acc: 0.3610  T-1 mAcc: 0.2303\n",
      "2020-01-06 07:49:11.721417-[Train]-Iter: 098535  Loss: 0.0581  T-1 Acc: 0.9456  T-1 mAcc: 0.4635\n",
      "2020-01-06 07:49:13.773739-[Train]-Iter: 098545  Loss: 0.0734  T-1 Acc: 0.8788  T-1 mAcc: 0.4405\n",
      "2020-01-06 07:49:15.827722-[Train]-Iter: 098555  Loss: 0.1105  T-1 Acc: 0.9425  T-1 mAcc: 0.4783\n",
      "2020-01-06 07:49:17.637473-[Train]-Iter: 098565  Loss: 0.0143  T-1 Acc: 0.9920  T-1 mAcc: 0.4931\n",
      "2020-01-06 07:49:19.321709-[Train]-Iter: 098575  Loss: 0.0433  T-1 Acc: 0.9196  T-1 mAcc: 0.4580\n",
      "2020-01-06 07:49:21.395509-[Train]-Iter: 098585  Loss: 0.0700  T-1 Acc: 0.9574  T-1 mAcc: 0.4804\n",
      "2020-01-06 07:49:23.478565-[Train]-Iter: 098595  Loss: 0.0250  T-1 Acc: 0.9603  T-1 mAcc: 0.4769\n",
      "2020-01-06 07:49:25.541460-[Train]-Iter: 098605  Loss: 0.0177  T-1 Acc: 0.9800  T-1 mAcc: 0.4877\n",
      "2020-01-06 07:49:27.606250-[Train]-Iter: 098615  Loss: 0.0332  T-1 Acc: 0.8604  T-1 mAcc: 0.2151\n",
      "2020-01-06 07:49:29.678701-[Train]-Iter: 098625  Loss: 0.0491  T-1 Acc: 0.8059  T-1 mAcc: 0.3218\n",
      "2020-01-06 07:49:31.754751-[Train]-Iter: 098635  Loss: 0.0156  T-1 Acc: 0.9822  T-1 mAcc: 0.4879\n",
      "2020-01-06 07:49:33.811428-[Train]-Iter: 098645  Loss: 0.0850  T-1 Acc: 0.9740  T-1 mAcc: 0.4754\n",
      "2020-01-06 07:49:35.870111-[Train]-Iter: 098655  Loss: 0.0234  T-1 Acc: 0.8755  T-1 mAcc: 0.2189\n",
      "2020-01-06 07:49:37.929585-[Train]-Iter: 098665  Loss: 0.0147  T-1 Acc: 0.9848  T-1 mAcc: 0.4876\n",
      "2020-01-06 07:49:40.015253-[Train]-Iter: 098675  Loss: 0.0477  T-1 Acc: 0.9253  T-1 mAcc: 0.4623\n",
      "2020-01-06 07:49:42.105021-[Train]-Iter: 098685  Loss: 0.0310  T-1 Acc: 0.8652  T-1 mAcc: 0.2163\n",
      "2020-01-06 07:49:44.151845-[Train]-Iter: 098695  Loss: 0.0071  T-1 Acc: 0.9835  T-1 mAcc: 0.4922\n",
      "2020-01-06 07:49:46.226665-[Train]-Iter: 098705  Loss: 0.0437  T-1 Acc: 0.9810  T-1 mAcc: 0.4861\n",
      "2020-01-06 07:49:48.285768-[Train]-Iter: 098715  Loss: 0.0628  T-1 Acc: 0.8933  T-1 mAcc: 0.4463\n",
      "2020-01-06 07:49:50.353452-[Train]-Iter: 098725  Loss: 0.0028  T-1 Acc: 0.9902  T-1 mAcc: 0.2476\n",
      "2020-01-06 07:49:52.420475-[Train]-Iter: 098735  Loss: 0.0541  T-1 Acc: 0.9069  T-1 mAcc: 0.4535\n",
      "2020-01-06 07:49:54.500558-[Train]-Iter: 098745  Loss: 0.0551  T-1 Acc: 0.9324  T-1 mAcc: 0.4514\n",
      "2020-01-06 07:49:56.583354-[Train]-Iter: 098755  Loss: 0.0269  T-1 Acc: 0.9746  T-1 mAcc: 0.4855\n",
      "2020-01-06 07:49:58.660210-[Train]-Iter: 098765  Loss: 0.0161  T-1 Acc: 0.9827  T-1 mAcc: 0.4912\n",
      "2020-01-06 07:50:00.726572-[Train]-Iter: 098775  Loss: 0.0320  T-1 Acc: 0.9652  T-1 mAcc: 0.4815\n",
      "2020-01-06 07:50:02.783904-[Train]-Iter: 098785  Loss: 0.0277  T-1 Acc: 0.9696  T-1 mAcc: 0.4798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-06 07:50:04.861761-[Train]-Iter: 098795  Loss: 0.0409  T-1 Acc: 0.9283  T-1 mAcc: 0.4543\n",
      "2020-01-06 07:50:06.945531-[Train]-Iter: 098805  Loss: 0.0192  T-1 Acc: 0.9815  T-1 mAcc: 0.4849\n",
      "2020-01-06 07:50:09.024642-[Train]-Iter: 098815  Loss: 0.0438  T-1 Acc: 0.9578  T-1 mAcc: 0.4775\n",
      "2020-01-06 07:50:11.105751-[Train]-Iter: 098825  Loss: 0.0425  T-1 Acc: 0.9485  T-1 mAcc: 0.4640\n",
      "2020-01-06 07:50:13.166793-[Train]-Iter: 098835  Loss: 0.0409  T-1 Acc: 0.9740  T-1 mAcc: 0.4824\n",
      "2020-01-06 07:50:15.232607-[Train]-Iter: 098845  Loss: 0.0346  T-1 Acc: 0.9670  T-1 mAcc: 0.4651\n",
      "2020-01-06 07:50:17.294205-[Train]-Iter: 098855  Loss: 0.0015  T-1 Acc: 0.9980  T-1 mAcc: 0.2495\n",
      "2020-01-06 07:50:19.354144-[Train]-Iter: 098865  Loss: 0.0310  T-1 Acc: 0.9597  T-1 mAcc: 0.4763\n",
      "2020-01-06 07:50:21.415599-[Train]-Iter: 098875  Loss: 0.0478  T-1 Acc: 0.9260  T-1 mAcc: 0.4627\n",
      "2020-01-06 07:50:23.482047-[Train]-Iter: 098885  Loss: 0.0322  T-1 Acc: 0.9418  T-1 mAcc: 0.4698\n",
      "2020-01-06 07:50:25.534679-[Train]-Iter: 098895  Loss: 0.0584  T-1 Acc: 0.9138  T-1 mAcc: 0.4558\n",
      "2020-01-06 07:50:27.621151-[Train]-Iter: 098905  Loss: 0.0553  T-1 Acc: 0.9267  T-1 mAcc: 0.4664\n",
      "2020-01-06 07:50:29.690284-[Train]-Iter: 098915  Loss: 0.1125  T-1 Acc: 0.8558  T-1 mAcc: 0.4381\n",
      "2020-01-06 07:50:31.757250-[Train]-Iter: 098925  Loss: 0.1292  T-1 Acc: 0.8508  T-1 mAcc: 0.4050\n",
      "2020-01-06 07:50:33.837281-[Train]-Iter: 098935  Loss: 0.0286  T-1 Acc: 0.9718  T-1 mAcc: 0.4774\n",
      "2020-01-06 07:50:35.899867-[Train]-Iter: 098945  Loss: 0.0163  T-1 Acc: 0.9845  T-1 mAcc: 0.4873\n",
      "2020-01-06 07:50:37.976501-[Train]-Iter: 098955  Loss: 0.0437  T-1 Acc: 0.9194  T-1 mAcc: 0.4613\n",
      "2020-01-06 07:50:40.050181-[Train]-Iter: 098965  Loss: 0.0955  T-1 Acc: 0.9483  T-1 mAcc: 0.4751\n",
      "2020-01-06 07:50:42.133445-[Train]-Iter: 098975  Loss: 0.0371  T-1 Acc: 0.9691  T-1 mAcc: 0.4521\n",
      "2020-01-06 07:50:44.205427-[Train]-Iter: 098985  Loss: 0.0777  T-1 Acc: 0.8506  T-1 mAcc: 0.4269\n",
      "2020-01-06 07:50:46.264373-[Train]-Iter: 098995  Loss: 0.0164  T-1 Acc: 0.9791  T-1 mAcc: 0.4830\n",
      "2020-01-06 07:50:48.324599-[Train]-Iter: 099005  Loss: 0.1420  T-1 Acc: 0.9298  T-1 mAcc: 0.4315\n",
      "2020-01-06 07:50:50.388692-[Train]-Iter: 099015  Loss: 0.0222  T-1 Acc: 0.9640  T-1 mAcc: 0.4778\n",
      "2020-01-06 07:50:52.450623-[Train]-Iter: 099025  Loss: 0.0178  T-1 Acc: 0.9776  T-1 mAcc: 0.4850\n",
      "2020-01-06 07:50:54.521495-[Train]-Iter: 099035  Loss: 0.0512  T-1 Acc: 0.9406  T-1 mAcc: 0.4704\n",
      "2020-01-06 07:50:56.597344-[Train]-Iter: 099045  Loss: 0.0452  T-1 Acc: 0.8985  T-1 mAcc: 0.4539\n",
      "2020-01-06 07:50:58.665163-[Train]-Iter: 099055  Loss: 0.0573  T-1 Acc: 0.8405  T-1 mAcc: 0.4231\n",
      "2020-01-06 07:51:00.732754-[Train]-Iter: 099065  Loss: 0.0405  T-1 Acc: 0.9384  T-1 mAcc: 0.4628\n",
      "2020-01-06 07:51:02.793621-[Train]-Iter: 099075  Loss: 0.1020  T-1 Acc: 0.9541  T-1 mAcc: 0.4740\n",
      "2020-01-06 07:51:04.862967-[Train]-Iter: 099085  Loss: 0.0095  T-1 Acc: 0.9836  T-1 mAcc: 0.4920\n",
      "2020-01-06 07:51:06.923672-[Train]-Iter: 099095  Loss: 0.0840  T-1 Acc: 0.8590  T-1 mAcc: 0.4186\n",
      "2020-01-06 07:51:08.990992-[Train]-Iter: 099105  Loss: 0.1625  T-1 Acc: 0.5233  T-1 mAcc: 0.2673\n",
      "2020-01-06 07:51:10.995344-[Train]-Iter: 099115  Loss: 0.0713  T-1 Acc: 0.9172  T-1 mAcc: 0.4516\n",
      "2020-01-06 07:51:13.054872-[Train]-Iter: 099125  Loss: 0.0009  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2020-01-06 07:51:15.117144-[Train]-Iter: 099135  Loss: 0.0459  T-1 Acc: 0.9451  T-1 mAcc: 0.4654\n",
      "2020-01-06 07:51:17.173884-[Train]-Iter: 099145  Loss: 0.2872  T-1 Acc: 0.2995  T-1 mAcc: 0.2432\n",
      "2020-01-06 07:51:19.241947-[Train]-Iter: 099155  Loss: 0.0597  T-1 Acc: 0.9221  T-1 mAcc: 0.4546\n",
      "2020-01-06 07:51:21.310619-[Train]-Iter: 099165  Loss: 0.1117  T-1 Acc: 0.9382  T-1 mAcc: 0.4664\n",
      "2020-01-06 07:51:23.381312-[Train]-Iter: 099175  Loss: 0.0933  T-1 Acc: 0.8274  T-1 mAcc: 0.4172\n",
      "2020-01-06 07:51:25.452766-[Train]-Iter: 099185  Loss: 0.0382  T-1 Acc: 0.8722  T-1 mAcc: 0.3408\n",
      "2020-01-06 07:51:27.522402-[Train]-Iter: 099195  Loss: 0.0501  T-1 Acc: 0.9772  T-1 mAcc: 0.4849\n",
      "2020-01-06 07:51:29.581573-[Train]-Iter: 099205  Loss: 0.0253  T-1 Acc: 0.9790  T-1 mAcc: 0.4872\n",
      "2020-01-06 07:51:31.650714-[Train]-Iter: 099215  Loss: 0.0526  T-1 Acc: 0.9203  T-1 mAcc: 0.4664\n",
      "2020-01-06 07:51:33.719633-[Train]-Iter: 099225  Loss: 0.0637  T-1 Acc: 0.9417  T-1 mAcc: 0.4725\n",
      "2020-01-06 07:51:35.779095-[Train]-Iter: 099235  Loss: 0.0251  T-1 Acc: 0.8955  T-1 mAcc: 0.2239\n",
      "2020-01-06 07:51:37.791801-[Train]-Iter: 099245  Loss: 0.0533  T-1 Acc: 0.9688  T-1 mAcc: 0.4811\n",
      "2020-01-06 07:51:39.846769-[Train]-Iter: 099255  Loss: 0.0430  T-1 Acc: 0.9268  T-1 mAcc: 0.4610\n",
      "2020-01-06 07:51:41.899229-[Train]-Iter: 099265  Loss: 0.0435  T-1 Acc: 0.9405  T-1 mAcc: 0.4697\n",
      "2020-01-06 07:51:43.959597-[Train]-Iter: 099275  Loss: 0.0822  T-1 Acc: 0.9545  T-1 mAcc: 0.4774\n",
      "2020-01-06 07:51:46.007871-[Train]-Iter: 099285  Loss: 0.0358  T-1 Acc: 0.9704  T-1 mAcc: 0.4844\n",
      "2020-01-06 07:51:48.076353-[Train]-Iter: 099295  Loss: 0.1015  T-1 Acc: 0.8671  T-1 mAcc: 0.4397\n",
      "2020-01-06 07:51:50.149220-[Train]-Iter: 099305  Loss: 0.0004  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2020-01-06 07:51:52.216609-[Train]-Iter: 099315  Loss: 0.0209  T-1 Acc: 0.9709  T-1 mAcc: 0.4838\n",
      "2020-01-06 07:51:54.268705-[Train]-Iter: 099325  Loss: 0.0317  T-1 Acc: 0.9555  T-1 mAcc: 0.4771\n",
      "2020-01-06 07:51:56.333568-[Train]-Iter: 099335  Loss: 0.0202  T-1 Acc: 0.9434  T-1 mAcc: 0.4786\n",
      "2020-01-06 07:51:58.395638-[Train]-Iter: 099345  Loss: 0.0948  T-1 Acc: 0.4668  T-1 mAcc: 0.1167\n",
      "2020-01-06 07:52:00.454565-[Train]-Iter: 099355  Loss: 0.0341  T-1 Acc: 0.9578  T-1 mAcc: 0.4754\n",
      "2020-01-06 07:52:02.531010-[Train]-Iter: 099365  Loss: 0.1558  T-1 Acc: 0.4153  T-1 mAcc: 0.2594\n",
      "2020-01-06 07:52:04.622011-[Train]-Iter: 099375  Loss: 0.0802  T-1 Acc: 0.8335  T-1 mAcc: 0.2084\n",
      "2020-01-06 07:52:06.712121-[Train]-Iter: 099385  Loss: 0.0019  T-1 Acc: 0.9971  T-1 mAcc: 0.2493\n",
      "2020-01-06 07:52:08.756108-[Train]-Iter: 099395  Loss: 0.1819  T-1 Acc: 0.4247  T-1 mAcc: 0.2428\n",
      "2020-01-06 07:52:10.805347-[Train]-Iter: 099405  Loss: 0.0462  T-1 Acc: 0.9425  T-1 mAcc: 0.4693\n",
      "2020-01-06 07:52:12.894557-[Train]-Iter: 099415  Loss: 0.0565  T-1 Acc: 0.9112  T-1 mAcc: 0.4554\n",
      "2020-01-06 07:52:14.965276-[Train]-Iter: 099425  Loss: 0.0918  T-1 Acc: 0.9682  T-1 mAcc: 0.4742\n",
      "2020-01-06 07:52:17.039164-[Train]-Iter: 099435  Loss: 0.3818  T-1 Acc: 0.2825  T-1 mAcc: 0.2426\n",
      "2020-01-06 07:52:19.111422-[Train]-Iter: 099445  Loss: 0.0829  T-1 Acc: 0.7688  T-1 mAcc: 0.3541\n",
      "2020-01-06 07:52:21.186524-[Train]-Iter: 099455  Loss: 0.0953  T-1 Acc: 0.9433  T-1 mAcc: 0.4282\n",
      "2020-01-06 07:52:23.261441-[Train]-Iter: 099465  Loss: 0.0432  T-1 Acc: 0.9656  T-1 mAcc: 0.4775\n",
      "2020-01-06 07:52:25.323245-[Train]-Iter: 099475  Loss: 0.0181  T-1 Acc: 0.9673  T-1 mAcc: 0.2418\n",
      "2020-01-06 07:52:27.392586-[Train]-Iter: 099485  Loss: 0.0305  T-1 Acc: 0.9643  T-1 mAcc: 0.4745\n",
      "2020-01-06 07:52:29.458193-[Train]-Iter: 099495  Loss: 0.0947  T-1 Acc: 0.6938  T-1 mAcc: 0.1735\n",
      "2020-01-06 07:52:31.519361-[Train]-Iter: 099505  Loss: 0.0531  T-1 Acc: 0.9377  T-1 mAcc: 0.4633\n",
      "INFO:tensorflow:/home/kartik/DL_model/PointCNN/outfolder_argo_gr3/ckpts/iter-99505 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "2020-01-06 07:52:32.513934-Checkpoint saved to /home/kartik/DL_model/PointCNN/outfolder_argo_gr3/ckpts/iter!\n",
      "2020-01-06 07:52:34.574283-[Train]-Iter: 099515  Loss: 0.0464  T-1 Acc: 0.7476  T-1 mAcc: 0.1869\n",
      "2020-01-06 07:52:36.640989-[Train]-Iter: 099525  Loss: 0.0931  T-1 Acc: 0.9683  T-1 mAcc: 0.4795\n",
      "2020-01-06 07:52:38.716624-[Train]-Iter: 099535  Loss: 0.0148  T-1 Acc: 0.9811  T-1 mAcc: 0.4874\n",
      "2020-01-06 07:52:40.784706-[Train]-Iter: 099545  Loss: 0.0091  T-1 Acc: 0.9575  T-1 mAcc: 0.2394\n",
      "2020-01-06 07:52:42.859133-[Train]-Iter: 099555  Loss: 0.0146  T-1 Acc: 0.9829  T-1 mAcc: 0.4860\n",
      "2020-01-06 07:52:44.946138-[Train]-Iter: 099565  Loss: 0.0224  T-1 Acc: 0.9760  T-1 mAcc: 0.4863\n",
      "2020-01-06 07:52:47.028055-[Train]-Iter: 099575  Loss: 0.0881  T-1 Acc: 0.5586  T-1 mAcc: 0.1396\n",
      "2020-01-06 07:52:49.109100-[Train]-Iter: 099585  Loss: 0.0892  T-1 Acc: 0.8966  T-1 mAcc: 0.4531\n",
      "2020-01-06 07:52:51.178194-[Train]-Iter: 099595  Loss: 0.0562  T-1 Acc: 0.8770  T-1 mAcc: 0.4340\n",
      "2020-01-06 07:52:53.254464-[Train]-Iter: 099605  Loss: 0.0134  T-1 Acc: 0.9827  T-1 mAcc: 0.4881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-06 07:52:55.327518-[Train]-Iter: 099615  Loss: 0.0138  T-1 Acc: 0.9905  T-1 mAcc: 0.4926\n",
      "2020-01-06 07:52:57.382028-[Train]-Iter: 099625  Loss: 0.0482  T-1 Acc: 0.9606  T-1 mAcc: 0.4728\n",
      "2020-01-06 07:52:59.455997-[Train]-Iter: 099635  Loss: 0.0206  T-1 Acc: 0.9650  T-1 mAcc: 0.4826\n",
      "2020-01-06 07:53:01.531057-[Train]-Iter: 099645  Loss: 0.0237  T-1 Acc: 0.9768  T-1 mAcc: 0.4810\n",
      "2020-01-06 07:53:03.597261-[Train]-Iter: 099655  Loss: 0.0004  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2020-01-06 07:53:05.661250-[Train]-Iter: 099665  Loss: 0.0757  T-1 Acc: 0.8712  T-1 mAcc: 0.4260\n",
      "2020-01-06 07:53:07.728999-[Train]-Iter: 099675  Loss: 0.0447  T-1 Acc: 0.9542  T-1 mAcc: 0.4679\n",
      "2020-01-06 07:53:09.795975-[Train]-Iter: 099685  Loss: 0.0247  T-1 Acc: 0.9632  T-1 mAcc: 0.4782\n",
      "2020-01-06 07:53:11.870434-[Train]-Iter: 099695  Loss: 0.0373  T-1 Acc: 0.9505  T-1 mAcc: 0.4736\n",
      "2020-01-06 07:53:13.943203-[Train]-Iter: 099705  Loss: 0.0399  T-1 Acc: 0.9609  T-1 mAcc: 0.4744\n",
      "2020-01-06 07:53:16.013840-[Train]-Iter: 099715  Loss: 0.0093  T-1 Acc: 0.9816  T-1 mAcc: 0.4910\n",
      "2020-01-06 07:53:18.093054-[Train]-Iter: 099725  Loss: 0.1699  T-1 Acc: 0.8341  T-1 mAcc: 0.4089\n",
      "2020-01-06 07:53:20.152426-[Train]-Iter: 099735  Loss: 0.1645  T-1 Acc: 0.7214  T-1 mAcc: 0.3630\n",
      "2020-01-06 07:53:22.225332-[Train]-Iter: 099745  Loss: 0.0557  T-1 Acc: 0.7090  T-1 mAcc: 0.1772\n",
      "2020-01-06 07:53:24.275355-[Train]-Iter: 099755  Loss: 0.0568  T-1 Acc: 0.9329  T-1 mAcc: 0.4711\n",
      "2020-01-06 07:53:26.333782-[Train]-Iter: 099765  Loss: 0.0519  T-1 Acc: 0.9592  T-1 mAcc: 0.4704\n",
      "2020-01-06 07:53:28.398345-[Train]-Iter: 099775  Loss: 0.0671  T-1 Acc: 0.9494  T-1 mAcc: 0.4741\n",
      "2020-01-06 07:53:30.454213-[Train]-Iter: 099785  Loss: 0.0296  T-1 Acc: 0.9621  T-1 mAcc: 0.4758\n",
      "2020-01-06 07:53:32.533341-[Train]-Iter: 099795  Loss: 0.0439  T-1 Acc: 0.9295  T-1 mAcc: 0.4574\n",
      "2020-01-06 07:53:34.605242-[Train]-Iter: 099805  Loss: 0.0232  T-1 Acc: 0.9691  T-1 mAcc: 0.4808\n",
      "2020-01-06 07:53:36.682008-[Train]-Iter: 099815  Loss: 0.0138  T-1 Acc: 0.9829  T-1 mAcc: 0.4858\n",
      "2020-01-06 07:53:38.765828-[Train]-Iter: 099825  Loss: 0.0698  T-1 Acc: 0.9381  T-1 mAcc: 0.4500\n",
      "2020-01-06 07:53:40.842486-[Train]-Iter: 099835  Loss: 0.0562  T-1 Acc: 0.8196  T-1 mAcc: 0.2416\n",
      "2020-01-06 07:53:42.912511-[Train]-Iter: 099845  Loss: 0.0013  T-1 Acc: 0.9995  T-1 mAcc: 0.2499\n",
      "2020-01-06 07:53:44.984634-[Train]-Iter: 099855  Loss: 0.0949  T-1 Acc: 0.8138  T-1 mAcc: 0.4112\n",
      "2020-01-06 07:53:47.054961-[Train]-Iter: 099865  Loss: 0.0483  T-1 Acc: 0.9618  T-1 mAcc: 0.4815\n",
      "2020-01-06 07:53:49.117584-[Train]-Iter: 099875  Loss: 0.0326  T-1 Acc: 0.8794  T-1 mAcc: 0.2198\n",
      "2020-01-06 07:53:51.184522-[Train]-Iter: 099885  Loss: 0.0102  T-1 Acc: 0.9772  T-1 mAcc: 0.4890\n",
      "2020-01-06 07:53:53.251843-[Train]-Iter: 099895  Loss: 0.4069  T-1 Acc: 0.4031  T-1 mAcc: 0.2435\n",
      "2020-01-06 07:53:55.329312-[Train]-Iter: 099905  Loss: 0.0169  T-1 Acc: 0.9782  T-1 mAcc: 0.4835\n",
      "2020-01-06 07:53:57.403615-[Train]-Iter: 099915  Loss: 0.0409  T-1 Acc: 0.9550  T-1 mAcc: 0.4612\n",
      "2020-01-06 07:53:59.476858-[Train]-Iter: 099925  Loss: 0.0176  T-1 Acc: 0.9897  T-1 mAcc: 0.4943\n",
      "2020-01-06 07:54:01.554896-[Train]-Iter: 099935  Loss: 0.1062  T-1 Acc: 0.8107  T-1 mAcc: 0.4051\n",
      "2020-01-06 07:54:03.630988-[Train]-Iter: 099945  Loss: 0.0160  T-1 Acc: 0.9824  T-1 mAcc: 0.4867\n",
      "2020-01-06 07:54:05.701515-[Train]-Iter: 099955  Loss: 0.0306  T-1 Acc: 0.9513  T-1 mAcc: 0.4760\n",
      "2020-01-06 07:54:07.797148-[Train]-Iter: 099965  Loss: 0.0800  T-1 Acc: 0.8508  T-1 mAcc: 0.4203\n",
      "2020-01-06 07:54:09.890048-[Train]-Iter: 099975  Loss: 0.1679  T-1 Acc: 0.7583  T-1 mAcc: 0.3804\n",
      "2020-01-06 07:54:11.884284-[Train]-Iter: 099985  Loss: 1.5778  T-1 Acc: 0.1309  T-1 mAcc: 0.2117\n",
      "2020-01-06 07:54:13.971919-[Train]-Iter: 099995  Loss: 0.0671  T-1 Acc: 0.9256  T-1 mAcc: 0.4483\n",
      "2020-01-06 07:54:16.053162-[Train]-Iter: 100005  Loss: 0.2492  T-1 Acc: 0.4285  T-1 mAcc: 0.2851\n",
      "2020-01-06 07:54:18.121268-[Train]-Iter: 100015  Loss: 0.0256  T-1 Acc: 0.8975  T-1 mAcc: 0.2244\n",
      "2020-01-06 07:54:20.203416-[Train]-Iter: 100025  Loss: 0.0273  T-1 Acc: 0.9598  T-1 mAcc: 0.4769\n",
      "2020-01-06 07:54:22.301559-[Train]-Iter: 100035  Loss: 0.0591  T-1 Acc: 0.9163  T-1 mAcc: 0.4521\n",
      "2020-01-06 07:54:24.376017-[Train]-Iter: 100045  Loss: 0.0443  T-1 Acc: 0.9517  T-1 mAcc: 0.4665\n",
      "2020-01-06 07:54:26.446171-[Train]-Iter: 100055  Loss: 0.0677  T-1 Acc: 0.9621  T-1 mAcc: 0.4715\n",
      "2020-01-06 07:54:27.923901-[Train]-Iter: 100065  Loss: 0.0467  T-1 Acc: 0.9660  T-1 mAcc: 0.4588\n",
      "2020-01-06 07:54:29.985950-[Train]-Iter: 100075  Loss: 0.0186  T-1 Acc: 0.9893  T-1 mAcc: 0.4916\n",
      "2020-01-06 07:54:32.070081-[Train]-Iter: 100085  Loss: 0.0552  T-1 Acc: 0.9717  T-1 mAcc: 0.4801\n",
      "2020-01-06 07:54:34.128168-[Train]-Iter: 100095  Loss: 0.0811  T-1 Acc: 0.8829  T-1 mAcc: 0.4415\n",
      "2020-01-06 07:54:36.175979-[Train]-Iter: 100105  Loss: 0.0694  T-1 Acc: 0.9683  T-1 mAcc: 0.4417\n",
      "2020-01-06 07:54:38.216542-[Train]-Iter: 100115  Loss: 0.0538  T-1 Acc: 0.8413  T-1 mAcc: 0.2103\n",
      "2020-01-06 07:54:40.246943-[Train]-Iter: 100125  Loss: 0.2837  T-1 Acc: 0.4369  T-1 mAcc: 0.2307\n",
      "2020-01-06 07:54:42.321903-[Train]-Iter: 100135  Loss: 0.0681  T-1 Acc: 0.8765  T-1 mAcc: 0.4335\n",
      "2020-01-06 07:54:44.401399-[Train]-Iter: 100145  Loss: 0.0456  T-1 Acc: 0.9716  T-1 mAcc: 0.4840\n",
      "2020-01-06 07:54:46.468470-[Train]-Iter: 100155  Loss: 0.0801  T-1 Acc: 0.9517  T-1 mAcc: 0.4765\n",
      "2020-01-06 07:54:48.558470-[Train]-Iter: 100165  Loss: 0.0375  T-1 Acc: 0.9498  T-1 mAcc: 0.4656\n",
      "2020-01-06 07:54:50.647998-[Train]-Iter: 100175  Loss: 0.0453  T-1 Acc: 0.8452  T-1 mAcc: 0.2113\n"
     ]
    }
   ],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\n",
    "\n",
    "with tf.Session( config=tf.ConfigProto(gpu_options=gpu_options) ) as sess:\n",
    "    summaries_op = tf.summary.merge_all('train')\n",
    "    summaries_val_op = tf.summary.merge_all('val')\n",
    "    summary_writer = tf.summary.FileWriter(folder_summary, sess.graph)\n",
    "\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Load the model\n",
    "    if args.load_ckpt is not None:\n",
    "        saver.restore(sess, args.load_ckpt)\n",
    "        print('{}-Checkpoint loaded from {}!'.format(datetime.now(), args.load_ckpt))\n",
    "    else:\n",
    "        latest_ckpt = tf.train.latest_checkpoint(folder_ckpt)\n",
    "        if latest_ckpt:\n",
    "            print('{}-Found checkpoint {}'.format(datetime.now(), latest_ckpt))\n",
    "            saver.restore(sess, latest_ckpt)\n",
    "            print('{}-Checkpoint loaded from {} (Iter {})'.format(\n",
    "                datetime.now(), latest_ckpt, sess.run(global_step)))\n",
    "\n",
    "    for batch_idx_train in range(batch_num):\n",
    "\n",
    "\n",
    "        ######################################################################\n",
    "        # Training\n",
    "        start_idx = (batch_size * batch_idx_train) % num_train\n",
    "        end_idx = min(start_idx + batch_size, num_train)\n",
    "        batch_size_train = end_idx - start_idx\n",
    "\n",
    "        points_batch = data_train[start_idx:end_idx, ...] # data number\n",
    "        points_num_batch = data_num_train[start_idx:end_idx, ...] # Number of data points \n",
    "        labels_batch = label_train[start_idx:end_idx, ...] # labels of segmentation for each point\n",
    "        weights_batch = np.array(label_weights_list)[labels_batch] \n",
    "\n",
    "\n",
    "        if start_idx + batch_size_train == num_train:\n",
    "#             if is_list_of_h5_list:\n",
    "#                 filelist_train_prev = seg_list[(seg_list_idx - 1) % len(seg_list)]\n",
    "#                 filelist_train = seg_list[seg_list_idx % len(seg_list)]\n",
    "#                 if filelist_train != filelist_train_prev:\n",
    "#                     # Load the train data and labels \n",
    "#                     data_train, _, data_num_train, label_train, _ = data_utils.load_seg(filelist_train)\n",
    "#                     num_train = data_train.shape[0]\n",
    "#                 seg_list_idx = seg_list_idx + 1\n",
    "            data_train, data_num_train, label_train = \\\n",
    "                data_utils.grouped_shuffle([data_train, data_num_train, label_train])\n",
    "\n",
    "        offset = int(random.gauss(0, sample_num * setting.sample_num_variance))\n",
    "        offset = max(offset, -sample_num * setting.sample_num_clip)\n",
    "        offset = min(offset, sample_num * setting.sample_num_clip)\n",
    "        sample_num_train = sample_num + offset\n",
    "        xforms_np, rotations_np = pf.get_xforms(batch_size_train,\n",
    "                                                rotation_range=rotation_range,\n",
    "                                                scaling_range=scaling_range,\n",
    "                                                order=setting.rotation_order)\n",
    "        sess.run(reset_metrics_op)\n",
    "        sess.run([train_op, loss_mean_update_op, t_1_acc_update_op, t_1_per_class_acc_update_op],\n",
    "                 feed_dict={\n",
    "                     pts_fts: points_batch,\n",
    "                     indices: pf.get_indices(batch_size_train, sample_num_train, points_num_batch),\n",
    "                     xforms: xforms_np,\n",
    "                     rotations: rotations_np,\n",
    "                     jitter_range: np.array([jitter]),\n",
    "                     labels_seg: labels_batch,\n",
    "                     labels_weights: weights_batch,\n",
    "                     is_training: True,\n",
    "                 })\n",
    "        if batch_idx_train % 10 == 0:\n",
    "            loss, t_1_acc, t_1_per_class_acc, summaries, step = sess.run([loss_mean_op,\n",
    "                                                                    t_1_acc_op,\n",
    "                                                                    t_1_per_class_acc_op,\n",
    "                                                                    summaries_op,\n",
    "                                                                    global_step])\n",
    "            summary_writer.add_summary(summaries, step)\n",
    "            print('{}-[Train]-Iter: {:06d}  Loss: {:.4f}  T-1 Acc: {:.4f}  T-1 mAcc: {:.4f}'\n",
    "                  .format(datetime.now(), step, loss, t_1_acc, t_1_per_class_acc))\n",
    "            sys.stdout.flush()\n",
    "        ######################################################################\n",
    "\n",
    "        if batch_idx_train % 1500 == 0:\n",
    "            filename_ckpt = os.path.join(folder_ckpt, 'iter')\n",
    "            saver.save(sess, filename_ckpt, global_step=global_step)\n",
    "            print('{}-Checkpoint saved to {}!'.format(datetime.now(), filename_ckpt))        \n",
    "        \n",
    "        \n",
    "        ######################################################################\n",
    "        # Validation\n",
    "        if (batch_idx_train % step_val == 0 and (batch_idx_train != 0 or args.load_ckpt is not None)) \\\n",
    "                or batch_idx_train == batch_num - 1:\n",
    "            filename_ckpt = os.path.join(folder_ckpt, 'iter')\n",
    "            saver.save(sess, filename_ckpt, global_step=global_step)\n",
    "            print('{}-Checkpoint saved to {}!'.format(datetime.now(), filename_ckpt))\n",
    "\n",
    "            sess.run(reset_metrics_op)\n",
    "            for batch_val_idx in range(batch_num_val):\n",
    "                start_idx = batch_size * batch_val_idx\n",
    "                end_idx = min(start_idx + batch_size, num_val)\n",
    "                batch_size_val = end_idx - start_idx\n",
    "                points_batch = data_val[start_idx:end_idx, ...]\n",
    "                points_num_batch = data_num_val[start_idx:end_idx, ...]\n",
    "                labels_batch = label_val[start_idx:end_idx, ...]\n",
    "                \n",
    "                weights_batch = np.array(label_weights_list)[labels_batch]\n",
    "\n",
    "                xforms_np, rotations_np = pf.get_xforms(batch_size_val,\n",
    "                                                        rotation_range=rotation_range_val,\n",
    "                                                        scaling_range=scaling_range_val,\n",
    "                                                        order=setting.rotation_order)\n",
    "                sess.run([loss_mean_update_op, t_1_acc_update_op, t_1_per_class_acc_update_op],\n",
    "                         feed_dict={\n",
    "                             pts_fts: points_batch,\n",
    "                             indices: pf.get_indices(batch_size_val, sample_num, points_num_batch),\n",
    "                             xforms: xforms_np,\n",
    "                             rotations: rotations_np,\n",
    "                             jitter_range: np.array([jitter_val]),\n",
    "                             labels_seg: labels_batch,\n",
    "                             labels_weights: weights_batch,\n",
    "                             is_training: False,\n",
    "                         })\n",
    "            loss_val, t_1_acc_val, t_1_per_class_acc_val, summaries_val, step = sess.run(\n",
    "                [loss_mean_op, t_1_acc_op, t_1_per_class_acc_op, summaries_val_op, global_step])\n",
    "            summary_writer.add_summary(summaries_val, step)\n",
    "            print('{}-[ Val ]-Average:      Loss: {:.4f}  T-1 Acc: {:.4f}  T-1 mAcc: {:.4f}'\n",
    "                  .format(datetime.now(), loss_val, t_1_acc_val, t_1_per_class_acc_val))\n",
    "            sys.stdout.flush()\n",
    "        ######################################################################\n",
    "    print('{}-Done!'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_ckpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
