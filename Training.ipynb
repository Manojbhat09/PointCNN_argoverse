{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import argparse\n",
    "import importlib\n",
    "import data_utils_legacy as data_utils\n",
    "import numpy as np\n",
    "import pointfly as pf\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, epochs=1, filelist='data/KITTI/ImageSets/train.txt', filelist_val='data/KITTI/ImageSets/train.txt', load_ckpt=None, log='log.txt', model='pointcnn_seg', no_code_backup=False, no_timestamp_folder=False, save_folder='/home/kartik/saved/', setting='kitti3d_x8_2048_fps')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--filelist', '-t', default=\"data/KITTI/ImageSets/train.txt\", help='Path to training set ground truth (.txt)')\n",
    "parser.add_argument('--filelist_val', '-v', default=\"data/KITTI/ImageSets/train.txt\", help='Path to validation set ground truth (.txt)')\n",
    "parser.add_argument('--load_ckpt', '-l', help='Path to a check point file for load')\n",
    "parser.add_argument('--save_folder', '-s', default=\"/home/kartik/saved/\", help='Path to folder for saving check points and summary')\n",
    "parser.add_argument('--model', '-m', default=\"pointcnn_seg\", help='Model to use')\n",
    "parser.add_argument('--setting', '-x', default=\"kitti3d_x8_2048_fps\", help='Setting to use')\n",
    "parser.add_argument('--epochs', default=\"1\",help='Number of training epochs (default defined in setting)', type=int)\n",
    "parser.add_argument('--batch_size', default=\"1\", help='Batch size (default defined in setting)', type=int)\n",
    "# default=\"64\",\n",
    "parser.add_argument('--log', help='Log to FILE in save folder; use - for stdout (default is log.txt)', metavar='FILE', default='log.txt')\n",
    "parser.add_argument('--no_timestamp_folder', help='Dont save to timestamp folder', action='store_true')\n",
    "parser.add_argument('--no_code_backup', help='Dont backup code', action='store_true')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "model = importlib.import_module(args.model)\n",
    "setting_path = os.path.join(cwd, args.model)\n",
    "sys.path.append(setting_path)\n",
    "setting = importlib.import_module(args.setting)\n",
    "\n",
    "num_epochs = args.epochs or setting.num_epochs\n",
    "batch_size = args.batch_size or setting.batch_size\n",
    "sample_num = setting.sample_num\n",
    "step_val = setting.step_val\n",
    "label_weights_list = setting.label_weights\n",
    "rotation_range = setting.rotation_range\n",
    "rotation_range_val = setting.rotation_range_val\n",
    "scaling_range = setting.scaling_range\n",
    "scaling_range_val = setting.scaling_range_val\n",
    "jitter = setting.jitter\n",
    "jitter_val = setting.jitter_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st1=\"val_zero_12.h5\"\n",
    "\n",
    "points = []\n",
    "labels = []\n",
    "point_nums = []\n",
    "labels_seg = []\n",
    "indices_split_to_full = []\n",
    "\n",
    "data = h5py.File(st1)\n",
    "points.append(data['data'][:,:,0:4].astype(np.float32))\n",
    "# labels.append(data['label'][...].astype(np.int64))\n",
    "point_nums.append(data['data_num'][...].astype(np.int32))\n",
    "labels_seg.append(data['label_seg'][...].astype(np.int64))\n",
    "if 'indices_split_to_full' in data:\n",
    "    indices_split_to_full.append(data['indices_split_to_full'][...].astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2004, 10000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label_seg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2004, 10000, 4), (2004,), (2004, 10000))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[0].shape, point_nums[0].shape, labels_seg[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[ -2.804,  -1.737,  -5.088,   0.   ],\n",
       "         [ -1.762,  -1.507,  -3.516,   0.   ],\n",
       "         [ -2.659,   0.524, -13.599,   0.   ],\n",
       "         ...,\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ]],\n",
       " \n",
       "        [[ 25.941,  -1.293,  -5.239,   0.   ],\n",
       "         [ 18.757,  -0.112, -12.696,   0.   ],\n",
       "         [  0.926,  -1.707,   4.321,   0.   ],\n",
       "         ...,\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ]],\n",
       " \n",
       "        [[ 15.051,  -1.614, -12.781,   0.   ],\n",
       "         [  4.534,  -1.683,  -1.397,   0.   ],\n",
       "         [ 15.641,  -1.535,   0.371,   0.   ],\n",
       "         ...,\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0.44 ,  -1.336,   6.285,   0.   ],\n",
       "         [ 32.308,  -1.242,   3.467,   0.   ],\n",
       "         [ 25.052,  -1.394,   6.213,   0.   ],\n",
       "         ...,\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ]],\n",
       " \n",
       "        [[ 40.746,  -1.715,  61.325,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         ...,\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ]],\n",
       " \n",
       "        [[  5.875,  -1.501,  -5.305,   0.   ],\n",
       "         [  3.662,  -1.389,  -6.908,   0.   ],\n",
       "         [ 10.752,  -0.449,  -7.28 ,   0.   ],\n",
       "         ...,\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ],\n",
       "         [  0.   ,   0.   ,   0.   ,   0.   ]]], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-28 14:32:42.487040-Preparing datasets...\n",
      "txt\n"
     ]
    }
   ],
   "source": [
    "print('{}-Preparing datasets...'.format(datetime.now()))\n",
    "# is_list_of_h5_list = data_utils.is_h5_list(args.filelist)\n",
    "args.filelist = os.path.join(cwd, \"data\", \"KITTI\", \"ImageSets\", \"val.txt\")\n",
    "# if is_list_of_h5_list:\n",
    "#     seg_list = data_utils.load_seg_list(args.filelist)\n",
    "#     seg_list_idx = 0\n",
    "#     filelist_train = seg_list[seg_list_idx]\n",
    "#     seg_list_idx = seg_list_idx + 1\n",
    "# else:\n",
    "print(\"txt\")\n",
    "\n",
    "filelist_train = args.filelist\n",
    "\n",
    "data_train, data_num_train, label_train =  points[0], point_nums[0], labels_seg[0]\n",
    "data_val, data_num_val, label_val = points[0], point_nums[0], labels_seg[0]\n",
    "\n",
    "# shuffle\n",
    "data_train, data_num_train, label_train = \\\n",
    "    data_utils.grouped_shuffle([data_train, data_num_train, label_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-28 14:32:43.838973-2004/2004 training/validation samples.\n",
      "2019-12-28 14:32:43.839051-2004 training batches.\n",
      "2019-12-28 14:32:43.839113-2004 testing batches per test.\n"
     ]
    }
   ],
   "source": [
    "num_train = data_train.shape[0]\n",
    "point_num = data_train.shape[1]\n",
    "num_val = data_val.shape[0]\n",
    "print('{}-{:d}/{:d} training/validation samples.'.format(datetime.now(), num_train, num_val))\n",
    "batch_num = (num_train * num_epochs + batch_size - 1) // batch_size\n",
    "print('{}-{:d} training batches.'.format(datetime.now(), batch_num))\n",
    "batch_num_val = math.ceil(num_val / batch_size)\n",
    "print('{}-{:d} testing batches per test.'.format(datetime.now(), batch_num_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Placeholders\n",
    "indices = tf.placeholder(tf.int32, shape=(None, None, 2), name=\"indices\")\n",
    "xforms = tf.placeholder(tf.float32, shape=(None, 3, 3), name=\"xforms\")\n",
    "rotations = tf.placeholder(tf.float32, shape=(None, 3, 3), name=\"rotations\")\n",
    "jitter_range = tf.placeholder(tf.float32, shape=(1), name=\"jitter_range\")\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "pts_fts = tf.placeholder(tf.float32, shape=(None, point_num, setting.data_dim), name='pts_fts')\n",
    "labels_seg = tf.placeholder(tf.int64, shape=(None, point_num), name='labels_seg')\n",
    "labels_weights = tf.placeholder(tf.float32, shape=(None, point_num), name='labels_weights')\n",
    "\n",
    "######################################################################\n",
    "pts_fts_sampled = tf.gather_nd(pts_fts, indices=indices, name='pts_fts_sampled')\n",
    "features_augmented = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setting.data_dim > 3:\n",
    "    points_sampled, features_sampled = tf.split(pts_fts_sampled,\n",
    "                                                [3, setting.data_dim - 3],\n",
    "                                                axis=-1,\n",
    "                                                name='split_points_features')\n",
    "    if setting.use_extra_features:\n",
    "        if setting.with_normal_feature:\n",
    "            if setting.data_dim < 6:\n",
    "                print('Only 3D normals are supported!')\n",
    "                exit()\n",
    "            elif setting.data_dim == 6:\n",
    "                features_augmented = pf.augment(features_sampled, rotations)\n",
    "            else:\n",
    "                normals, rest = tf.split(features_sampled, [3, setting.data_dim - 6])\n",
    "                normals_augmented = pf.augment(normals, rotations)\n",
    "                features_augmented = tf.concat([normals_augmented, rest], axis=-1)\n",
    "        else:\n",
    "            features_augmented = features_sampled\n",
    "else:\n",
    "    points_sampled = pts_fts_sampled\n",
    "points_augmented = pf.augment(points_sampled, xforms, jitter_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kartik/DL_model/PointCNN/pointfly.py:194: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "labels_sampled = tf.gather_nd(labels_seg, indices=indices, name='labels_sampled')\n",
    "labels_weights_sampled = tf.gather_nd(labels_weights, indices=indices, name='labels_weight_sampled')\n",
    "\n",
    "net = model.Net(points_augmented, features_augmented, is_training, setting)\n",
    "logits = net.logits\n",
    "probs = tf.nn.softmax(logits, name='probs')\n",
    "predictions = tf.argmax(probs, axis=-1, name='predictions')\n",
    "\n",
    "loss_op = tf.losses.sparse_softmax_cross_entropy(labels=labels_sampled, logits=logits,\n",
    "                                                 weights=labels_weights_sampled)\n",
    "\n",
    "with tf.name_scope('metrics'):\n",
    "    loss_mean_op, loss_mean_update_op = tf.metrics.mean(loss_op)\n",
    "    t_1_acc_op, t_1_acc_update_op = tf.metrics.accuracy(labels_sampled, predictions, weights=labels_weights_sampled)\n",
    "    t_1_per_class_acc_op, t_1_per_class_acc_update_op = \\\n",
    "        tf.metrics.mean_per_class_accuracy(labels_sampled, predictions, setting.num_class,\n",
    "                                           weights=labels_weights_sampled)\n",
    "reset_metrics_op = tf.variables_initializer([var for var in tf.local_variables()\n",
    "                                             if var.name.split('/')[0] == 'metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tf.summary.scalar('loss/train', tensor=loss_mean_op, collections=['train'])\n",
    "_ = tf.summary.scalar('t_1_acc/train', tensor=t_1_acc_op, collections=['train'])\n",
    "_ = tf.summary.scalar('t_1_per_class_acc/train', tensor=t_1_per_class_acc_op, collections=['train'])\n",
    "\n",
    "_ = tf.summary.scalar('loss/val', tensor=loss_mean_op, collections=['val'])\n",
    "_ = tf.summary.scalar('t_1_acc/val', tensor=t_1_acc_op, collections=['val'])\n",
    "_ = tf.summary.scalar('t_1_per_class_acc/val', tensor=t_1_per_class_acc_op, collections=['val'])\n",
    "\n",
    "lr_exp_op = tf.train.exponential_decay(setting.learning_rate_base, global_step, setting.decay_steps,\n",
    "                                       setting.decay_rate, staircase=True)\n",
    "lr_clip_op = tf.maximum(lr_exp_op, setting.learning_rate_min)\n",
    "_ = tf.summary.scalar('learning_rate', tensor=lr_clip_op, collections=['train'])\n",
    "reg_loss = setting.weight_decay * tf.losses.get_regularization_loss()\n",
    "if setting.optimizer == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr_clip_op, epsilon=setting.epsilon)\n",
    "elif setting.optimizer == 'momentum':\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=lr_clip_op, momentum=setting.momentum, use_nesterov=True)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss_op + reg_loss, global_step=global_step)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-28 14:33:08.458939-Parameter number: 11510276.\n"
     ]
    }
   ],
   "source": [
    "# backup all code\n",
    "# if not args.no_code_backup:\n",
    "#     code_folder = os.path.abspath(os.path.dirname(__file__))\n",
    "#     shutil.copytree(code_folder, os.path.join(root_folder, os.path.basename(code_folder)))\n",
    "\n",
    "root_folder = os.path.join(cwd, \"outfolder\")\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "# shutil.copytree(code_folder, os.path.join(root_folder, os.path.basename(code_folder)))\n",
    "\n",
    "folder_ckpt = os.path.join(root_folder, 'ckpts')\n",
    "if not os.path.exists(folder_ckpt):\n",
    "    os.makedirs(folder_ckpt)\n",
    "\n",
    "folder_summary = os.path.join(root_folder, 'summary')\n",
    "if not os.path.exists(folder_summary):\n",
    "    os.makedirs(folder_summary)\n",
    "\n",
    "parameter_num = np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])\n",
    "print('{}-Parameter number: {:d}.'.format(datetime.now(), parameter_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-28 14:42:40.484029-Found checkpoint /home/kartik/DL_model/PointCNN/outfolder/ckpts/iter-2001\n",
      "INFO:tensorflow:Restoring parameters from /home/kartik/DL_model/PointCNN/outfolder/ckpts/iter-2001\n",
      "2019-12-28 14:42:40.879883-Checkpoint loaded from /home/kartik/DL_model/PointCNN/outfolder/ckpts/iter-2001 (Iter 2001)\n",
      "2019-12-28 14:42:49.223077-[Train]-Iter: 002002  Loss: 0.0015  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:42:49.990248-[Train]-Iter: 002012  Loss: 0.0017  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:42:50.756365-[Train]-Iter: 002022  Loss: 0.0035  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:42:51.522172-[Train]-Iter: 002032  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:42:52.287496-[Train]-Iter: 002042  Loss: 0.0021  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:42:53.059664-[Train]-Iter: 002052  Loss: 0.0013  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:42:53.831338-[Train]-Iter: 002062  Loss: 0.0170  T-1 Acc: 0.9797  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:42:54.601120-[Train]-Iter: 002072  Loss: 0.0127  T-1 Acc: 0.9878  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:42:55.373199-[Train]-Iter: 002082  Loss: 0.0140  T-1 Acc: 0.9837  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:42:56.144768-[Train]-Iter: 002092  Loss: 0.0085  T-1 Acc: 0.9538  T-1 mAcc: 0.3250\n",
      "2019-12-28 14:42:56.916046-[Train]-Iter: 002102  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:42:57.686515-[Train]-Iter: 002112  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:42:58.460521-[Train]-Iter: 002122  Loss: 0.0021  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:42:59.231114-[Train]-Iter: 002132  Loss: 0.0001  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:00.001443-[Train]-Iter: 002142  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:43:00.773519-[Train]-Iter: 002152  Loss: 0.0042  T-1 Acc: 0.9896  T-1 mAcc: 0.4998\n",
      "2019-12-28 14:43:01.543662-[Train]-Iter: 002162  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:43:02.316860-[Train]-Iter: 002172  Loss: 0.0012  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:03.091440-[Train]-Iter: 002182  Loss: 0.0017  T-1 Acc: 0.9953  T-1 mAcc: 0.4688\n",
      "2019-12-28 14:43:03.862708-[Train]-Iter: 002192  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:43:04.632344-[Train]-Iter: 002202  Loss: 0.0782  T-1 Acc: 0.7531  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:43:05.406557-[Train]-Iter: 002212  Loss: 0.0011  T-1 Acc: 0.9952  T-1 mAcc: 0.3750\n",
      "2019-12-28 14:43:06.180012-[Train]-Iter: 002222  Loss: 0.0009  T-1 Acc: 0.9903  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:43:06.952837-[Train]-Iter: 002232  Loss: 0.0065  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:07.730883-[Train]-Iter: 002242  Loss: 0.0066  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:08.502641-[Train]-Iter: 002252  Loss: 0.0020  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:09.275986-[Train]-Iter: 002262  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:43:10.050290-[Train]-Iter: 002272  Loss: 0.0017  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:10.824909-[Train]-Iter: 002282  Loss: 0.0013  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:11.601090-[Train]-Iter: 002292  Loss: 0.0001  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:43:12.375644-[Train]-Iter: 002302  Loss: 0.0001  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:13.147313-[Train]-Iter: 002312  Loss: 0.0248  T-1 Acc: 0.9400  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:43:13.918571-[Train]-Iter: 002322  Loss: 0.0167  T-1 Acc: 0.9759  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:14.689003-[Train]-Iter: 002332  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:43:15.464022-[Train]-Iter: 002342  Loss: 0.0010  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:16.236820-[Train]-Iter: 002352  Loss: 0.0015  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:17.009185-[Train]-Iter: 002362  Loss: 0.0006  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:17.783711-[Train]-Iter: 002372  Loss: 0.0834  T-1 Acc: 0.9138  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:18.556302-[Train]-Iter: 002382  Loss: 0.0012  T-1 Acc: 0.9991  T-1 mAcc: 0.4998\n",
      "2019-12-28 14:43:19.331327-[Train]-Iter: 002392  Loss: 0.0232  T-1 Acc: 0.9486  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:20.105772-[Train]-Iter: 002402  Loss: 0.0018  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:20.880464-[Train]-Iter: 002412  Loss: 0.0062  T-1 Acc: 0.9993  T-1 mAcc: 0.4997\n",
      "2019-12-28 14:43:21.656046-[Train]-Iter: 002422  Loss: 0.0033  T-1 Acc: 0.9956  T-1 mAcc: 0.4907\n",
      "2019-12-28 14:43:22.430236-[Train]-Iter: 002432  Loss: 0.0008  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:23.203446-[Train]-Iter: 002442  Loss: 0.0003  T-1 Acc: 0.9995  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:43:23.976775-[Train]-Iter: 002452  Loss: 0.0303  T-1 Acc: 0.9558  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:24.748976-[Train]-Iter: 002462  Loss: 0.0009  T-1 Acc: 0.9996  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:43:25.524005-[Train]-Iter: 002472  Loss: 0.0552  T-1 Acc: 0.9028  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:43:26.298660-[Train]-Iter: 002482  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:43:27.072295-[Train]-Iter: 002492  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:43:27.846439-[Train]-Iter: 002502  Loss: 0.0007  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "INFO:tensorflow:/home/kartik/DL_model/PointCNN/outfolder/ckpts/iter-2502 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "2019-12-28 14:43:28.993639-Checkpoint saved to /home/kartik/DL_model/PointCNN/outfolder/ckpts/iter!\n",
      "2019-12-28 14:44:54.028242-[Val  ]-Average:      Loss: 0.0062  T-1 Acc: 0.9885  T-1 mAcc: 0.5003\n",
      "2019-12-28 14:44:54.803930-[Train]-Iter: 002512  Loss: 0.0010  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:44:55.577551-[Train]-Iter: 002522  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:44:56.350645-[Train]-Iter: 002532  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:44:57.126084-[Train]-Iter: 002542  Loss: 0.0032  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:44:57.900964-[Train]-Iter: 002552  Loss: 0.0009  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:44:58.677924-[Train]-Iter: 002562  Loss: 0.0079  T-1 Acc: 0.9638  T-1 mAcc: 0.4256\n",
      "2019-12-28 14:44:59.451890-[Train]-Iter: 002572  Loss: 0.0002  T-1 Acc: 0.9995  T-1 mAcc: 0.2499\n",
      "2019-12-28 14:45:00.224679-[Train]-Iter: 002582  Loss: 0.0015  T-1 Acc: 0.9996  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:45:00.999548-[Train]-Iter: 002592  Loss: 0.0398  T-1 Acc: 0.9685  T-1 mAcc: 0.4997\n",
      "2019-12-28 14:45:01.772608-[Train]-Iter: 002602  Loss: 0.0058  T-1 Acc: 0.9988  T-1 mAcc: 0.4996\n",
      "2019-12-28 14:45:02.547774-[Train]-Iter: 002612  Loss: 0.0028  T-1 Acc: 0.9996  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:45:03.321671-[Train]-Iter: 002622  Loss: 0.0017  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:04.094542-[Train]-Iter: 002632  Loss: 0.0010  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:04.867469-[Train]-Iter: 002642  Loss: 0.0148  T-1 Acc: 0.9400  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:05.640507-[Train]-Iter: 002652  Loss: 0.0005  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:06.418266-[Train]-Iter: 002662  Loss: 0.0042  T-1 Acc: 0.9768  T-1 mAcc: 0.3958\n",
      "2019-12-28 14:45:07.194730-[Train]-Iter: 002672  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:07.969900-[Train]-Iter: 002682  Loss: 0.0022  T-1 Acc: 0.9996  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:45:08.743987-[Train]-Iter: 002692  Loss: 0.0012  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:09.520535-[Train]-Iter: 002702  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:10.296794-[Train]-Iter: 002712  Loss: 0.0003  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:11.072854-[Train]-Iter: 002722  Loss: 0.0001  T-1 Acc: 0.9995  T-1 mAcc: 0.2499\n",
      "2019-12-28 14:45:11.849039-[Train]-Iter: 002732  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:12.626387-[Train]-Iter: 002742  Loss: 0.0131  T-1 Acc: 0.9623  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:13.402529-[Train]-Iter: 002752  Loss: 0.0034  T-1 Acc: 0.9955  T-1 mAcc: 0.4886\n",
      "2019-12-28 14:45:14.178282-[Train]-Iter: 002762  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:14.956427-[Train]-Iter: 002772  Loss: 0.0003  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-28 14:45:15.733503-[Train]-Iter: 002782  Loss: 0.0017  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:16.507773-[Train]-Iter: 002792  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:17.284673-[Train]-Iter: 002802  Loss: 0.0053  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:18.060953-[Train]-Iter: 002812  Loss: 0.0004  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:18.839223-[Train]-Iter: 002822  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:19.614195-[Train]-Iter: 002832  Loss: 0.0026  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:20.389726-[Train]-Iter: 002842  Loss: 0.0081  T-1 Acc: 0.9673  T-1 mAcc: 0.3250\n",
      "2019-12-28 14:45:21.166133-[Train]-Iter: 002852  Loss: 0.0020  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:21.943131-[Train]-Iter: 002862  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:22.718167-[Train]-Iter: 002872  Loss: 0.0008  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:23.495960-[Train]-Iter: 002882  Loss: 0.0013  T-1 Acc: 0.9986  T-1 mAcc: 0.4996\n",
      "2019-12-28 14:45:24.271981-[Train]-Iter: 002892  Loss: 0.0002  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:25.045755-[Train]-Iter: 002902  Loss: 0.0006  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:25.820450-[Train]-Iter: 002912  Loss: 0.0059  T-1 Acc: 0.9866  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:26.597488-[Train]-Iter: 002922  Loss: 0.0042  T-1 Acc: 0.9821  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:27.371404-[Train]-Iter: 002932  Loss: 0.0028  T-1 Acc: 0.9815  T-1 mAcc: 0.4231\n",
      "2019-12-28 14:45:28.147424-[Train]-Iter: 002942  Loss: 0.0244  T-1 Acc: 0.9681  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:45:28.923018-[Train]-Iter: 002952  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:29.698608-[Train]-Iter: 002962  Loss: 0.0037  T-1 Acc: 0.9997  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:45:30.476617-[Train]-Iter: 002972  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:31.255220-[Train]-Iter: 002982  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:45:32.029455-[Train]-Iter: 002992  Loss: 0.0016  T-1 Acc: 0.9856  T-1 mAcc: 0.3125\n",
      "2019-12-28 14:45:32.805362-[Train]-Iter: 003002  Loss: 0.0030  T-1 Acc: 0.9987  T-1 mAcc: 0.4996\n",
      "INFO:tensorflow:/home/kartik/DL_model/PointCNN/outfolder/ckpts/iter-3002 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "2019-12-28 14:45:33.657895-Checkpoint saved to /home/kartik/DL_model/PointCNN/outfolder/ckpts/iter!\n",
      "2019-12-28 14:46:56.701947-[Val  ]-Average:      Loss: 0.2687  T-1 Acc: 0.9630  T-1 mAcc: 0.4631\n",
      "2019-12-28 14:46:57.496794-[Train]-Iter: 003012  Loss: 0.0005  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:46:58.273287-[Train]-Iter: 003022  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:46:59.049023-[Train]-Iter: 003032  Loss: 0.0112  T-1 Acc: 0.9720  T-1 mAcc: 0.4650\n",
      "2019-12-28 14:46:59.825105-[Train]-Iter: 003042  Loss: 0.0113  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:00.598788-[Train]-Iter: 003052  Loss: 0.0029  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:01.373236-[Train]-Iter: 003062  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:47:02.147870-[Train]-Iter: 003072  Loss: 0.1638  T-1 Acc: 0.7655  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:47:02.923689-[Train]-Iter: 003082  Loss: 0.0280  T-1 Acc: 0.9154  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:03.698382-[Train]-Iter: 003092  Loss: 0.0139  T-1 Acc: 0.9939  T-1 mAcc: 0.4967\n",
      "2019-12-28 14:47:04.475284-[Train]-Iter: 003102  Loss: 0.0025  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:05.250494-[Train]-Iter: 003112  Loss: 0.0073  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:06.026043-[Train]-Iter: 003122  Loss: 0.0003  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:06.802778-[Train]-Iter: 003132  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:07.577728-[Train]-Iter: 003142  Loss: 0.0011  T-1 Acc: 0.9991  T-1 mAcc: 0.4998\n",
      "2019-12-28 14:47:08.353975-[Train]-Iter: 003152  Loss: 0.0047  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:09.127992-[Train]-Iter: 003162  Loss: 0.0020  T-1 Acc: 0.9932  T-1 mAcc: 0.2483\n",
      "2019-12-28 14:47:09.904425-[Train]-Iter: 003172  Loss: 0.0001  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:47:10.680223-[Train]-Iter: 003182  Loss: 0.0003  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:11.453993-[Train]-Iter: 003192  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:47:12.230706-[Train]-Iter: 003202  Loss: 0.0010  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:13.007258-[Train]-Iter: 003212  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:47:13.781300-[Train]-Iter: 003222  Loss: 0.0013  T-1 Acc: 0.9987  T-1 mAcc: 0.4996\n",
      "2019-12-28 14:47:14.557237-[Train]-Iter: 003232  Loss: 0.0015  T-1 Acc: 0.9938  T-1 mAcc: 0.4580\n",
      "2019-12-28 14:47:15.333173-[Train]-Iter: 003242  Loss: 0.0054  T-1 Acc: 0.9866  T-1 mAcc: 0.4643\n",
      "2019-12-28 14:47:16.106777-[Train]-Iter: 003252  Loss: 0.0053  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:16.883712-[Train]-Iter: 003262  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:47:17.658797-[Train]-Iter: 003272  Loss: 0.0017  T-1 Acc: 0.9992  T-1 mAcc: 0.4998\n",
      "2019-12-28 14:47:18.435773-[Train]-Iter: 003282  Loss: 0.0010  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:19.211743-[Train]-Iter: 003292  Loss: 0.0014  T-1 Acc: 0.9992  T-1 mAcc: 0.4997\n",
      "2019-12-28 14:47:19.988793-[Train]-Iter: 003302  Loss: 0.0115  T-1 Acc: 0.9771  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:20.765841-[Train]-Iter: 003312  Loss: 0.0204  T-1 Acc: 0.9715  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:21.542646-[Train]-Iter: 003322  Loss: 0.0016  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:22.318057-[Train]-Iter: 003332  Loss: 0.0011  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:23.094741-[Train]-Iter: 003342  Loss: 0.0028  T-1 Acc: 0.9906  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:23.869522-[Train]-Iter: 003352  Loss: 0.0031  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:24.647136-[Train]-Iter: 003362  Loss: 0.0052  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:25.425572-[Train]-Iter: 003372  Loss: 0.0004  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:26.202568-[Train]-Iter: 003382  Loss: 0.1010  T-1 Acc: 0.8604  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:26.976739-[Train]-Iter: 003392  Loss: 0.0019  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:27.753724-[Train]-Iter: 003402  Loss: 0.0568  T-1 Acc: 0.8816  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:28.530391-[Train]-Iter: 003412  Loss: 0.0033  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:29.306731-[Train]-Iter: 003422  Loss: 0.0004  T-1 Acc: 0.9995  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:47:30.083753-[Train]-Iter: 003432  Loss: 0.0532  T-1 Acc: 0.8725  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:47:30.859367-[Train]-Iter: 003442  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:47:31.634846-[Train]-Iter: 003452  Loss: 0.0008  T-1 Acc: 0.9996  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:47:32.410722-[Train]-Iter: 003462  Loss: 0.0002  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:33.186698-[Train]-Iter: 003472  Loss: 0.0014  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:33.962625-[Train]-Iter: 003482  Loss: 0.0002  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:47:34.737321-[Train]-Iter: 003492  Loss: 0.0004  T-1 Acc: 0.9995  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:47:35.512515-[Train]-Iter: 003502  Loss: 0.0011  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "INFO:tensorflow:/home/kartik/DL_model/PointCNN/outfolder/ckpts/iter-3502 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "2019-12-28 14:47:36.383411-Checkpoint saved to /home/kartik/DL_model/PointCNN/outfolder/ckpts/iter!\n",
      "2019-12-28 14:49:01.217094-[Val  ]-Average:      Loss: 0.0064  T-1 Acc: 0.9881  T-1 mAcc: 0.4998\n",
      "2019-12-28 14:49:02.028101-[Train]-Iter: 003512  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:02.802053-[Train]-Iter: 003522  Loss: 0.0188  T-1 Acc: 0.9735  T-1 mAcc: 0.4997\n",
      "2019-12-28 14:49:03.578764-[Train]-Iter: 003532  Loss: 0.0044  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:04.352391-[Train]-Iter: 003542  Loss: 0.0011  T-1 Acc: 0.9986  T-1 mAcc: 0.4996\n",
      "2019-12-28 14:49:05.128027-[Train]-Iter: 003552  Loss: 0.0097  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-28 14:49:05.904083-[Train]-Iter: 003562  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:06.680216-[Train]-Iter: 003572  Loss: 0.0005  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:07.455732-[Train]-Iter: 003582  Loss: 0.0022  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:08.227904-[Train]-Iter: 003592  Loss: 0.0076  T-1 Acc: 0.9883  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:09.004767-[Train]-Iter: 003602  Loss: 0.0013  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:09.780357-[Train]-Iter: 003612  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:10.555612-[Train]-Iter: 003622  Loss: 0.0026  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:11.329475-[Train]-Iter: 003632  Loss: 0.0289  T-1 Acc: 0.9208  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:12.105386-[Train]-Iter: 003642  Loss: 0.0262  T-1 Acc: 0.9772  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:12.882368-[Train]-Iter: 003652  Loss: 0.0054  T-1 Acc: 0.9898  T-1 mAcc: 0.4844\n",
      "2019-12-28 14:49:13.656517-[Train]-Iter: 003662  Loss: 0.0009  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:14.433101-[Train]-Iter: 003672  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:15.208291-[Train]-Iter: 003682  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:15.982976-[Train]-Iter: 003692  Loss: 0.0004  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:16.760612-[Train]-Iter: 003702  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:17.537185-[Train]-Iter: 003712  Loss: 0.0059  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:18.312030-[Train]-Iter: 003722  Loss: 0.0103  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:19.087196-[Train]-Iter: 003732  Loss: 0.0035  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:19.864736-[Train]-Iter: 003742  Loss: 0.0093  T-1 Acc: 0.9820  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:20.639614-[Train]-Iter: 003752  Loss: 0.0013  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:21.414532-[Train]-Iter: 003762  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:22.191692-[Train]-Iter: 003772  Loss: 0.0022  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:22.969800-[Train]-Iter: 003782  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:23.744791-[Train]-Iter: 003792  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:24.520743-[Train]-Iter: 003802  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:25.299109-[Train]-Iter: 003812  Loss: 0.0032  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:26.074440-[Train]-Iter: 003822  Loss: 0.0003  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:26.852401-[Train]-Iter: 003832  Loss: 0.0007  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:27.628941-[Train]-Iter: 003842  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:28.404557-[Train]-Iter: 003852  Loss: 0.0065  T-1 Acc: 0.9765  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:29.177916-[Train]-Iter: 003862  Loss: 0.0067  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:29.954788-[Train]-Iter: 003872  Loss: 0.0006  T-1 Acc: 0.9995  T-1 mAcc: 0.4999\n",
      "2019-12-28 14:49:30.730352-[Train]-Iter: 003882  Loss: 0.0044  T-1 Acc: 0.9984  T-1 mAcc: 0.4995\n",
      "2019-12-28 14:49:31.507197-[Train]-Iter: 003892  Loss: 0.0008  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:32.282694-[Train]-Iter: 003902  Loss: 0.0042  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:33.060480-[Train]-Iter: 003912  Loss: 0.0001  T-1 Acc: 0.9995  T-1 mAcc: 0.2499\n",
      "2019-12-28 14:49:33.836920-[Train]-Iter: 003922  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:34.614126-[Train]-Iter: 003932  Loss: 0.1185  T-1 Acc: 0.7591  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:35.392283-[Train]-Iter: 003942  Loss: 0.0252  T-1 Acc: 0.9667  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:36.167612-[Train]-Iter: 003952  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:36.942451-[Train]-Iter: 003962  Loss: 0.0033  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:37.719551-[Train]-Iter: 003972  Loss: 0.0016  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:38.497302-[Train]-Iter: 003982  Loss: 0.0006  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "2019-12-28 14:49:39.275020-[Train]-Iter: 003992  Loss: 0.0000  T-1 Acc: 1.0000  T-1 mAcc: 0.2500\n",
      "2019-12-28 14:49:40.052196-[Train]-Iter: 004002  Loss: 0.0003  T-1 Acc: 1.0000  T-1 mAcc: 0.5000\n",
      "INFO:tensorflow:/home/kartik/DL_model/PointCNN/outfolder/ckpts/iter-4002 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "2019-12-28 14:49:40.881970-Checkpoint saved to /home/kartik/DL_model/PointCNN/outfolder/ckpts/iter!\n",
      "2019-12-28 14:51:05.862864-[Val  ]-Average:      Loss: 0.0076  T-1 Acc: 0.9870  T-1 mAcc: 0.4992\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'is_list_of_h5_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-20f919df14a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnum_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mis_list_of_h5_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                 \u001b[0mfilelist_train_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_list_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mfilelist_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseg_list_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseg_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_list_of_h5_list' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    summaries_op = tf.summary.merge_all('train')\n",
    "    summaries_val_op = tf.summary.merge_all('val')\n",
    "    summary_writer = tf.summary.FileWriter(folder_summary, sess.graph)\n",
    "\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Load the model\n",
    "    if args.load_ckpt is not None:\n",
    "        saver.restore(sess, args.load_ckpt)\n",
    "        print('{}-Checkpoint loaded from {}!'.format(datetime.now(), args.load_ckpt))\n",
    "    else:\n",
    "        latest_ckpt = tf.train.latest_checkpoint(folder_ckpt)\n",
    "        if latest_ckpt:\n",
    "            print('{}-Found checkpoint {}'.format(datetime.now(), latest_ckpt))\n",
    "            saver.restore(sess, latest_ckpt)\n",
    "            print('{}-Checkpoint loaded from {} (Iter {})'.format(\n",
    "                datetime.now(), latest_ckpt, sess.run(global_step)))\n",
    "\n",
    "    for batch_idx_train in range(batch_num):\n",
    "\n",
    "\n",
    "        ######################################################################\n",
    "        # Training\n",
    "        start_idx = (batch_size * batch_idx_train) % num_train\n",
    "        end_idx = min(start_idx + batch_size, num_train)\n",
    "        batch_size_train = end_idx - start_idx\n",
    "\n",
    "        points_batch = data_train[start_idx:end_idx, ...] # data number\n",
    "        points_num_batch = data_num_train[start_idx:end_idx, ...] # Number of data points \n",
    "        labels_batch = label_train[start_idx:end_idx, ...] # labels of segmentation for each point\n",
    "        weights_batch = np.array(label_weights_list)[labels_batch] \n",
    "\n",
    "\n",
    "        if start_idx + batch_size_train == num_train:\n",
    "            if is_list_of_h5_list:\n",
    "                filelist_train_prev = seg_list[(seg_list_idx - 1) % len(seg_list)]\n",
    "                filelist_train = seg_list[seg_list_idx % len(seg_list)]\n",
    "                if filelist_train != filelist_train_prev:\n",
    "                    # Load the train data and labels \n",
    "                    data_train, _, data_num_train, label_train, _ = data_utils.load_seg(filelist_train)\n",
    "                    num_train = data_train.shape[0]\n",
    "                seg_list_idx = seg_list_idx + 1\n",
    "            data_train, data_num_train, label_train = \\\n",
    "                data_utils.grouped_shuffle([data_train, data_num_train, label_train])\n",
    "\n",
    "        offset = int(random.gauss(0, sample_num * setting.sample_num_variance))\n",
    "        offset = max(offset, -sample_num * setting.sample_num_clip)\n",
    "        offset = min(offset, sample_num * setting.sample_num_clip)\n",
    "        sample_num_train = sample_num + offset\n",
    "        xforms_np, rotations_np = pf.get_xforms(batch_size_train,\n",
    "                                                rotation_range=rotation_range,\n",
    "                                                scaling_range=scaling_range,\n",
    "                                                order=setting.rotation_order)\n",
    "        sess.run(reset_metrics_op)\n",
    "        sess.run([train_op, loss_mean_update_op, t_1_acc_update_op, t_1_per_class_acc_update_op],\n",
    "                 feed_dict={\n",
    "                     pts_fts: points_batch,\n",
    "                     indices: pf.get_indices(batch_size_train, sample_num_train, points_num_batch),\n",
    "                     xforms: xforms_np,\n",
    "                     rotations: rotations_np,\n",
    "                     jitter_range: np.array([jitter]),\n",
    "                     labels_seg: labels_batch,\n",
    "                     labels_weights: weights_batch,\n",
    "                     is_training: True,\n",
    "                 })\n",
    "        if batch_idx_train % 10 == 0:\n",
    "            loss, t_1_acc, t_1_per_class_acc, summaries, step = sess.run([loss_mean_op,\n",
    "                                                                    t_1_acc_op,\n",
    "                                                                    t_1_per_class_acc_op,\n",
    "                                                                    summaries_op,\n",
    "                                                                    global_step])\n",
    "            summary_writer.add_summary(summaries, step)\n",
    "            print('{}-[Train]-Iter: {:06d}  Loss: {:.4f}  T-1 Acc: {:.4f}  T-1 mAcc: {:.4f}'\n",
    "                  .format(datetime.now(), step, loss, t_1_acc, t_1_per_class_acc))\n",
    "            sys.stdout.flush()\n",
    "        ######################################################################\n",
    "\n",
    "        ######################################################################\n",
    "        # Validation\n",
    "        if (batch_idx_train % step_val == 0 and (batch_idx_train != 0 or args.load_ckpt is not None)) \\\n",
    "                or batch_idx_train == batch_num - 1:\n",
    "            filename_ckpt = os.path.join(folder_ckpt, 'iter')\n",
    "            saver.save(sess, filename_ckpt, global_step=global_step)\n",
    "            print('{}-Checkpoint saved to {}!'.format(datetime.now(), filename_ckpt))\n",
    "\n",
    "            sess.run(reset_metrics_op)\n",
    "            for batch_val_idx in range(batch_num_val):\n",
    "                start_idx = batch_size * batch_val_idx\n",
    "                end_idx = min(start_idx + batch_size, num_val)\n",
    "                batch_size_val = end_idx - start_idx\n",
    "                points_batch = data_val[start_idx:end_idx, ...]\n",
    "                points_num_batch = data_num_val[start_idx:end_idx, ...]\n",
    "                labels_batch = label_val[start_idx:end_idx, ...]\n",
    "                weights_batch = np.array(label_weights_list)[labels_batch]\n",
    "\n",
    "                xforms_np, rotations_np = pf.get_xforms(batch_size_val,\n",
    "                                                        rotation_range=rotation_range_val,\n",
    "                                                        scaling_range=scaling_range_val,\n",
    "                                                        order=setting.rotation_order)\n",
    "                sess.run([loss_mean_update_op, t_1_acc_update_op, t_1_per_class_acc_update_op],\n",
    "                         feed_dict={\n",
    "                             pts_fts: points_batch,\n",
    "                             indices: pf.get_indices(batch_size_val, sample_num, points_num_batch),\n",
    "                             xforms: xforms_np,\n",
    "                             rotations: rotations_np,\n",
    "                             jitter_range: np.array([jitter_val]),\n",
    "                             labels_seg: labels_batch,\n",
    "                             labels_weights: weights_batch,\n",
    "                             is_training: False,\n",
    "                         })\n",
    "            loss_val, t_1_acc_val, t_1_per_class_acc_val, summaries_val, step = sess.run(\n",
    "                [loss_mean_op, t_1_acc_op, t_1_per_class_acc_op, summaries_val_op, global_step])\n",
    "            summary_writer.add_summary(summaries_val, step)\n",
    "            print('{}-[Val  ]-Average:      Loss: {:.4f}  T-1 Acc: {:.4f}  T-1 mAcc: {:.4f}'\n",
    "                  .format(datetime.now(), loss_val, t_1_acc_val, t_1_per_class_acc_val))\n",
    "            sys.stdout.flush()\n",
    "        ######################################################################\n",
    "    print('{}-Done!'.format(datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
