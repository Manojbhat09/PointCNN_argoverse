{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "import argparse\n",
    "import importlib\n",
    "import data_utils_legacy as data_utils\n",
    "import numpy as np\n",
    "import pointfly as pf\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, epochs=1, filelist='data/KITTI/ImageSets/train.txt', filelist_val='data/KITTI/ImageSets/train.txt', load_ckpt=None, log='log.txt', model='pointcnn_seg', no_code_backup=False, no_timestamp_folder=False, save_folder='/home/kartik/saved/', setting='kitti3d_x8_2048_fps')\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--filelist', '-t', default=\"data/KITTI/ImageSets/train.txt\", help='Path to training set ground truth (.txt)')\n",
    "parser.add_argument('--filelist_val', '-v', default=\"data/KITTI/ImageSets/train.txt\", help='Path to validation set ground truth (.txt)')\n",
    "parser.add_argument('--load_ckpt', '-l', help='Path to a check point file for load')\n",
    "parser.add_argument('--save_folder', '-s', default=\"/home/kartik/saved/\", help='Path to folder for saving check points and summary')\n",
    "parser.add_argument('--model', '-m', default=\"pointcnn_seg\", help='Model to use')\n",
    "parser.add_argument('--setting', '-x', default=\"kitti3d_x8_2048_fps\", help='Setting to use')\n",
    "parser.add_argument('--epochs', default=\"1\",help='Number of training epochs (default defined in setting)', type=int)\n",
    "parser.add_argument('--batch_size', default=\"1\", help='Batch size (default defined in setting)', type=int)\n",
    "# default=\"64\",\n",
    "parser.add_argument('--log', help='Log to FILE in save folder; use - for stdout (default is log.txt)', metavar='FILE', default='log.txt')\n",
    "parser.add_argument('--no_timestamp_folder', help='Dont save to timestamp folder', action='store_true')\n",
    "parser.add_argument('--no_code_backup', help='Dont backup code', action='store_true')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "model = importlib.import_module(args.model)\n",
    "setting_path = os.path.join(cwd, args.model)\n",
    "sys.path.append(setting_path)\n",
    "setting = importlib.import_module(args.setting)\n",
    "\n",
    "num_epochs = args.epochs or setting.num_epochs\n",
    "batch_size = args.batch_size or setting.batch_size\n",
    "sample_num = setting.sample_num\n",
    "step_val = setting.step_val\n",
    "label_weights_list = setting.label_weights\n",
    "rotation_range = setting.rotation_range\n",
    "rotation_range_val = setting.rotation_range_val\n",
    "scaling_range = setting.scaling_range\n",
    "scaling_range_val = setting.scaling_range_val\n",
    "jitter = setting.jitter\n",
    "jitter_val = setting.jitter_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-28 19:27:34.597806-Preparing datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:05,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amax [[[214.27092   58.53326  190.85304 ]\n",
      "  [216.28676   82.44576  161.3603  ]\n",
      "  [217.7764    76.41629  213.24043 ]\n",
      "  ...\n",
      "  [121.73541   19.222141  82.93084 ]\n",
      "  [168.15007   55.433903  64.44276 ]\n",
      "  [ 96.57542    8.047347  73.72162 ]]]\n",
      "amin [[[-2.15250595e+02 -4.59061670e+00 -2.10146469e+02]\n",
      "  [-2.15206833e+02 -8.08369064e+00 -2.08944229e+02]\n",
      "  [-2.15246536e+02 -3.99475718e+00 -2.12600876e+02]\n",
      "  ...\n",
      "  [-1.49590454e+02 -1.83444142e-01 -1.07086624e+02]\n",
      "  [-8.59085846e+01 -1.22849226e+00 -1.16281860e+02]\n",
      "  [-1.22644920e+02 -1.13854599e+00 -3.25866127e+01]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:01,  3.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amax [[[208.28293    58.53326   123.89611  ]\n",
      "  [214.49594    82.44576   139.11711  ]\n",
      "  [213.06453    76.41629   125.840065 ]\n",
      "  ...\n",
      "  [ 71.913155    8.1823845  82.93084  ]\n",
      "  [ 39.420105   55.433903   64.44276  ]\n",
      "  [ 23.146116    8.047347   73.72162  ]]]\n",
      "amin [[[-2.0650757e+02 -2.1255884e+00 -2.0921796e+02]\n",
      "  [-2.1373587e+02 -1.9900390e+00 -2.0894423e+02]\n",
      "  [-2.1303616e+02 -2.2990389e+00 -2.1260088e+02]\n",
      "  ...\n",
      "  [-1.4959045e+02 -1.8344414e-01 -4.4655857e+01]\n",
      "  [-3.1115355e+01 -1.2284923e+00 -1.1628186e+02]\n",
      "  [-2.3069124e+01  0.0000000e+00 -6.0209899e+00]]]\n"
     ]
    }
   ],
   "source": [
    "st1=\"val_zero_12.h5\"\n",
    "\n",
    "points = []\n",
    "labels = []\n",
    "point_nums = []\n",
    "labels_seg = []\n",
    "indices_split_to_full = []\n",
    "\n",
    "print('{}-Preparing datasets...'.format(datetime.now()))\n",
    "# is_list_of_h5_list = data_utils.is_h5_list(args.filelist)\n",
    "args.filelist = os.path.join(cwd, \"data\", \"Argo_h5\", \"train\")+\"/train.txt\"\n",
    "args.filelist_val = os.path.join(cwd, \"data\", \"Argo_h5\", \"train\")+\"/val.txt\"\n",
    "\n",
    "is_list_of_h5_list = not data_utils.is_h5_list(args.filelist)\n",
    "if is_list_of_h5_list:\n",
    "    seg_list = data_utils.load_seg_list(args.filelist)\n",
    "    seg_list_idx = 0\n",
    "    filelist_train = seg_list[seg_list_idx]\n",
    "    seg_list_idx = seg_list_idx + 1\n",
    "else:\n",
    "    filelist_train = args.filelist\n",
    "data_train, _, data_num_train, label_train, _ = data_utils.load_seg_Argo(filelist_train)\n",
    "data_val, _, data_num_val, label_val, _ = data_utils.load_seg_Argo(args.filelist_val)\n",
    "\n",
    "\n",
    "data_train = np.dstack([data_train, label_train[:, :,np.newaxis]])\n",
    "data_val = np.dstack([data_val, label_val[:, :,np.newaxis]])\n",
    "\n",
    "# shuffle\n",
    "data_train, data_num_train, label_train = \\\n",
    "    data_utils.grouped_shuffle([data_train, data_num_train, label_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-28 20:35:18.743197-43942/10894 training/validation samples.\n",
      "2019-12-28 20:35:18.743278-43942 training batches.\n",
      "2019-12-28 20:35:18.743342-10894 testing batches per test.\n"
     ]
    }
   ],
   "source": [
    "num_train = data_train.shape[0]\n",
    "point_num = data_train.shape[1]\n",
    "num_val = data_val.shape[0]\n",
    "print('{}-{:d}/{:d} training/validation samples.'.format(datetime.now(), num_train, num_val))\n",
    "batch_num = (num_train * num_epochs + batch_size - 1) // batch_size\n",
    "print('{}-{:d} training batches.'.format(datetime.now(), batch_num))\n",
    "batch_num_val = math.ceil(num_val / batch_size)\n",
    "print('{}-{:d} testing batches per test.'.format(datetime.now(), batch_num_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################\n",
    "# Placeholders\n",
    "indices = tf.placeholder(tf.int32, shape=(None, None, 2), name=\"indices\")\n",
    "xforms = tf.placeholder(tf.float32, shape=(None, 3, 3), name=\"xforms\")\n",
    "rotations = tf.placeholder(tf.float32, shape=(None, 3, 3), name=\"rotations\")\n",
    "jitter_range = tf.placeholder(tf.float32, shape=(1), name=\"jitter_range\")\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "pts_fts = tf.placeholder(tf.float32, shape=(None, point_num, setting.data_dim), name='pts_fts')\n",
    "labels_seg = tf.placeholder(tf.int64, shape=(None, point_num), name='labels_seg')\n",
    "labels_weights = tf.placeholder(tf.float32, shape=(None, point_num), name='labels_weights')\n",
    "\n",
    "######################################################################\n",
    "pts_fts_sampled = tf.gather_nd(pts_fts, indices=indices, name='pts_fts_sampled')\n",
    "features_augmented = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if setting.data_dim > 3:\n",
    "    points_sampled, features_sampled = tf.split(pts_fts_sampled,\n",
    "                                                [3, setting.data_dim - 3],\n",
    "                                                axis=-1,\n",
    "                                                name='split_points_features')\n",
    "    if setting.use_extra_features:\n",
    "        if setting.with_normal_feature:\n",
    "            if setting.data_dim < 6:\n",
    "                print('Only 3D normals are supported!')\n",
    "                exit()\n",
    "            elif setting.data_dim == 6:\n",
    "                features_augmented = pf.augment(features_sampled, rotations)\n",
    "            else:\n",
    "                normals, rest = tf.split(features_sampled, [3, setting.data_dim - 6])\n",
    "                normals_augmented = pf.augment(normals, rotations)\n",
    "                features_augmented = tf.concat([normals_augmented, rest], axis=-1)\n",
    "        else:\n",
    "            features_augmented = features_sampled\n",
    "else:\n",
    "    points_sampled = pts_fts_sampled\n",
    "points_augmented = pf.augment(points_sampled, xforms, jitter_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kartik/DL_model/PointCNN/pointfly.py:194: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "labels_sampled = tf.gather_nd(labels_seg, indices=indices, name='labels_sampled')\n",
    "labels_weights_sampled = tf.gather_nd(labels_weights, indices=indices, name='labels_weight_sampled')\n",
    "\n",
    "net = model.Net(points_augmented, features_augmented, is_training, setting)\n",
    "logits = net.logits\n",
    "probs = tf.nn.softmax(logits, name='probs')\n",
    "predictions = tf.argmax(probs, axis=-1, name='predictions')\n",
    "\n",
    "loss_op = tf.losses.sparse_softmax_cross_entropy(labels=labels_sampled, logits=logits,\n",
    "                                                 weights=labels_weights_sampled)\n",
    "\n",
    "with tf.name_scope('metrics'):\n",
    "    loss_mean_op, loss_mean_update_op = tf.metrics.mean(loss_op)\n",
    "    t_1_acc_op, t_1_acc_update_op = tf.metrics.accuracy(labels_sampled, predictions, weights=labels_weights_sampled)\n",
    "    t_1_per_class_acc_op, t_1_per_class_acc_update_op = \\\n",
    "        tf.metrics.mean_per_class_accuracy(labels_sampled, predictions, setting.num_class,\n",
    "                                           weights=labels_weights_sampled)\n",
    "reset_metrics_op = tf.variables_initializer([var for var in tf.local_variables()\n",
    "                                             if var.name.split('/')[0] == 'metrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tf.summary.scalar('loss/train', tensor=loss_mean_op, collections=['train'])\n",
    "_ = tf.summary.scalar('t_1_acc/train', tensor=t_1_acc_op, collections=['train'])\n",
    "_ = tf.summary.scalar('t_1_per_class_acc/train', tensor=t_1_per_class_acc_op, collections=['train'])\n",
    "\n",
    "_ = tf.summary.scalar('loss/val', tensor=loss_mean_op, collections=['val'])\n",
    "_ = tf.summary.scalar('t_1_acc/val', tensor=t_1_acc_op, collections=['val'])\n",
    "_ = tf.summary.scalar('t_1_per_class_acc/val', tensor=t_1_per_class_acc_op, collections=['val'])\n",
    "\n",
    "lr_exp_op = tf.train.exponential_decay(setting.learning_rate_base, global_step, setting.decay_steps,\n",
    "                                       setting.decay_rate, staircase=True)\n",
    "lr_clip_op = tf.maximum(lr_exp_op, setting.learning_rate_min)\n",
    "_ = tf.summary.scalar('learning_rate', tensor=lr_clip_op, collections=['train'])\n",
    "reg_loss = setting.weight_decay * tf.losses.get_regularization_loss()\n",
    "if setting.optimizer == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr_clip_op, epsilon=setting.epsilon)\n",
    "elif setting.optimizer == 'momentum':\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=lr_clip_op, momentum=setting.momentum, use_nesterov=True)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    train_op = optimizer.minimize(loss_op + reg_loss, global_step=global_step)\n",
    "\n",
    "init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-28 19:28:07.399052-Parameter number: 11513360.\n"
     ]
    }
   ],
   "source": [
    "# backup all code\n",
    "# if not args.no_code_backup:\n",
    "#     code_folder = os.path.abspath(os.path.dirname(__file__))\n",
    "#     shutil.copytree(code_folder, os.path.join(root_folder, os.path.basename(code_folder)))\n",
    "\n",
    "root_folder = os.path.join(cwd, \"outfolder_argo\")\n",
    "if not os.path.exists(root_folder):\n",
    "    os.makedirs(root_folder)\n",
    "# shutil.copytree(code_folder, os.path.join(root_folder, os.path.basename(code_folder)))\n",
    "\n",
    "folder_ckpt = os.path.join(root_folder, 'ckpts')\n",
    "if not os.path.exists(folder_ckpt):\n",
    "    os.makedirs(folder_ckpt)\n",
    "\n",
    "folder_summary = os.path.join(root_folder, 'summary')\n",
    "if not os.path.exists(folder_summary):\n",
    "    os.makedirs(folder_summary)\n",
    "\n",
    "parameter_num = np.sum([np.prod(v.shape.as_list()) for v in tf.trainable_variables()])\n",
    "print('{}-Parameter number: {:d}.'.format(datetime.now(), parameter_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3319f5d72bbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0msummaries_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msummaries_val_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msummary_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_summary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    summaries_op = tf.summary.merge_all('train')\n",
    "    summaries_val_op = tf.summary.merge_all('val')\n",
    "    summary_writer = tf.summary.FileWriter(folder_summary, sess.graph)\n",
    "\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Load the model\n",
    "    if args.load_ckpt is not None:\n",
    "        saver.restore(sess, args.load_ckpt)\n",
    "        print('{}-Checkpoint loaded from {}!'.format(datetime.now(), args.load_ckpt))\n",
    "    else:\n",
    "        latest_ckpt = tf.train.latest_checkpoint(folder_ckpt)\n",
    "        if latest_ckpt:\n",
    "            print('{}-Found checkpoint {}'.format(datetime.now(), latest_ckpt))\n",
    "            saver.restore(sess, latest_ckpt)\n",
    "            print('{}-Checkpoint loaded from {} (Iter {})'.format(\n",
    "                datetime.now(), latest_ckpt, sess.run(global_step)))\n",
    "\n",
    "    for batch_idx_train in range(batch_num):\n",
    "\n",
    "\n",
    "        ######################################################################\n",
    "        # Training\n",
    "        start_idx = (batch_size * batch_idx_train) % num_train\n",
    "        end_idx = min(start_idx + batch_size, num_train)\n",
    "        batch_size_train = end_idx - start_idx\n",
    "\n",
    "        points_batch = data_train[start_idx:end_idx, ...] # data number\n",
    "        points_num_batch = data_num_train[start_idx:end_idx, ...] # Number of data points \n",
    "        labels_batch = label_train[start_idx:end_idx, ...] # labels of segmentation for each point\n",
    "        weights_batch = np.array(label_weights_list)[labels_batch] \n",
    "\n",
    "\n",
    "        if start_idx + batch_size_train == num_train:\n",
    "            if is_list_of_h5_list:\n",
    "                filelist_train_prev = seg_list[(seg_list_idx - 1) % len(seg_list)]\n",
    "                filelist_train = seg_list[seg_list_idx % len(seg_list)]\n",
    "                if filelist_train != filelist_train_prev:\n",
    "                    # Load the train data and labels \n",
    "                    data_train, _, data_num_train, label_train, _ = data_utils.load_seg(filelist_train)\n",
    "                    num_train = data_train.shape[0]\n",
    "                seg_list_idx = seg_list_idx + 1\n",
    "            data_train, data_num_train, label_train = \\\n",
    "                data_utils.grouped_shuffle([data_train, data_num_train, label_train])\n",
    "\n",
    "        offset = int(random.gauss(0, sample_num * setting.sample_num_variance))\n",
    "        offset = max(offset, -sample_num * setting.sample_num_clip)\n",
    "        offset = min(offset, sample_num * setting.sample_num_clip)\n",
    "        sample_num_train = sample_num + offset\n",
    "        xforms_np, rotations_np = pf.get_xforms(batch_size_train,\n",
    "                                                rotation_range=rotation_range,\n",
    "                                                scaling_range=scaling_range,\n",
    "                                                order=setting.rotation_order)\n",
    "        sess.run(reset_metrics_op)\n",
    "        sess.run([train_op, loss_mean_update_op, t_1_acc_update_op, t_1_per_class_acc_update_op],\n",
    "                 feed_dict={\n",
    "                     pts_fts: points_batch,\n",
    "                     indices: pf.get_indices(batch_size_train, sample_num_train, points_num_batch),\n",
    "                     xforms: xforms_np,\n",
    "                     rotations: rotations_np,\n",
    "                     jitter_range: np.array([jitter]),\n",
    "                     labels_seg: labels_batch,\n",
    "                     labels_weights: weights_batch,\n",
    "                     is_training: True,\n",
    "                 })\n",
    "        if batch_idx_train % 10 == 0:\n",
    "            loss, t_1_acc, t_1_per_class_acc, summaries, step = sess.run([loss_mean_op,\n",
    "                                                                    t_1_acc_op,\n",
    "                                                                    t_1_per_class_acc_op,\n",
    "                                                                    summaries_op,\n",
    "                                                                    global_step])\n",
    "            summary_writer.add_summary(summaries, step)\n",
    "            print('{}-[Train]-Iter: {:06d}  Loss: {:.4f}  T-1 Acc: {:.4f}  T-1 mAcc: {:.4f}'\n",
    "                  .format(datetime.now(), step, loss, t_1_acc, t_1_per_class_acc))\n",
    "            sys.stdout.flush()\n",
    "        ######################################################################\n",
    "\n",
    "        if batch_idx_train % 500 == 0:\n",
    "            filename_ckpt = os.path.join(folder_ckpt, 'iter')\n",
    "            saver.save(sess, filename_ckpt, global_step=global_step)\n",
    "            print('{}-Checkpoint saved to {}!'.format(datetime.now(), filename_ckpt))        \n",
    "        \n",
    "        \n",
    "        ######################################################################\n",
    "        # Validation\n",
    "#         if (batch_idx_train % step_val == 0 and (batch_idx_train != 0 or args.load_ckpt is not None)) \\\n",
    "#                 or batch_idx_train == batch_num - 1:\n",
    "#             filename_ckpt = os.path.join(folder_ckpt, 'iter')\n",
    "#             saver.save(sess, filename_ckpt, global_step=global_step)\n",
    "#             print('{}-Checkpoint saved to {}!'.format(datetime.now(), filename_ckpt))\n",
    "\n",
    "#             sess.run(reset_metrics_op)\n",
    "#             for batch_val_idx in range(batch_num_val):\n",
    "#                 start_idx = batch_size * batch_val_idx\n",
    "#                 end_idx = min(start_idx + batch_size, num_val)\n",
    "#                 batch_size_val = end_idx - start_idx\n",
    "#                 points_batch = data_val[start_idx:end_idx, ...]\n",
    "#                 points_num_batch = data_num_val[start_idx:end_idx, ...]\n",
    "#                 labels_batch = label_val[start_idx:end_idx, ...]\n",
    "                \n",
    "#                 weights_batch = np.array(label_weights_list)[labels_batch]\n",
    "\n",
    "#                 xforms_np, rotations_np = pf.get_xforms(batch_size_val,\n",
    "#                                                         rotation_range=rotation_range_val,\n",
    "#                                                         scaling_range=scaling_range_val,\n",
    "#                                                         order=setting.rotation_order)\n",
    "#                 sess.run([loss_mean_update_op, t_1_acc_update_op, t_1_per_class_acc_update_op],\n",
    "#                          feed_dict={\n",
    "#                              pts_fts: points_batch,\n",
    "#                              indices: pf.get_indices(batch_size_val, sample_num, points_num_batch),\n",
    "#                              xforms: xforms_np,\n",
    "#                              rotations: rotations_np,\n",
    "#                              jitter_range: np.array([jitter_val]),\n",
    "#                              labels_seg: labels_batch,\n",
    "#                              labels_weights: weights_batch,\n",
    "#                              is_training: False,\n",
    "#                          })\n",
    "#             loss_val, t_1_acc_val, t_1_per_class_acc_val, summaries_val, step = sess.run(\n",
    "#                 [loss_mean_op, t_1_acc_op, t_1_per_class_acc_op, summaries_val_op, global_step])\n",
    "#             summary_writer.add_summary(summaries_val, step)\n",
    "#             print('{}-[Val  ]-Average:      Loss: {:.4f}  T-1 Acc: {:.4f}  T-1 mAcc: {:.4f}'\n",
    "#                   .format(datetime.now(), loss_val, t_1_acc_val, t_1_per_class_acc_val))\n",
    "#             sys.stdout.flush()\n",
    "        ######################################################################\n",
    "    print('{}-Done!'.format(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kartik/DL_model/PointCNN/outfolder_argo/ckpts/iter'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_ckpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
